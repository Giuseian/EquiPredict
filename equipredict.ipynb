{"cells":[{"cell_type":"markdown","metadata":{},"source":["# EquiPredict: Robust Interaction Modeling for Multi-Agent Motion Forecasting\n","The ability to understand and predict the motion of multiple agents in dynamic environments is crucial for a range of applications, from autonomous vehicles navigating urban streets to sophisticated pedestrian traffic management systems. Despite considerable advancements, existing motion prediction methods often struggle to capture the intricate interdependencies and the variability these agents exhibit in real-world settings. \n","A fundamental challenge lies in ensuring that predictions remain consistent (equivariance) under Euclidean transformations and that interactions among agents are invariant to these transformations. \n","\n","Building on the principles established by EqMotion, we propose a refined model, \"EquiPredict: Robust Interaction Modeling for Multi-Agent Motion Forecasting,\" which specifically enhances the prediction of pedestrian trajectories. This re-implementation focuses on achieving high fidelity in motion forecasting by integrating an equivariant geometric feature learning module with an invariant interaction reasoning module. Our model aims to provide a robust and reliable framework specifically tuned to understand and predict pedestrian dynamics, addressing the nuanced movements and interactions typical in crowded urban environments."]},{"cell_type":"markdown","metadata":{},"source":["## All necessary Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-20T06:17:28.237600Z","iopub.status.busy":"2024-07-20T06:17:28.237210Z","iopub.status.idle":"2024-07-20T06:17:45.850509Z","shell.execute_reply":"2024-07-20T06:17:45.849373Z","shell.execute_reply.started":"2024-07-20T06:17:28.237568Z"},"trusted":true},"outputs":[],"source":["! pip install torch-geometric"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-20T06:17:45.854150Z","iopub.status.busy":"2024-07-20T06:17:45.853201Z","iopub.status.idle":"2024-07-20T06:17:53.220327Z","shell.execute_reply":"2024-07-20T06:17:53.219398Z","shell.execute_reply.started":"2024-07-20T06:17:45.854105Z"},"trusted":true},"outputs":[],"source":["import argparse\n","import torch\n","import os\n","from torch import nn, optim\n","import json\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","import math\n","import random\n","import sys \n","from torch import nn\n","import torch.nn.functional as F\n","import math\n","import cv2\n","import glob\n","import copy\n","import warnings\n","import torch.nn.init as init\n","from sklearn.cluster import KMeans\n","from torch_geometric.nn import GCNConv, GATConv \n","import matplotlib.animation as animation\n","from IPython.display import HTML\n","import torch.optim as optim\n","import pickle\n","warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")\n","sys.path.append(\"..\")"]},{"cell_type":"markdown","metadata":{},"source":["## Data Handling"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-20T06:17:53.234520Z","iopub.status.busy":"2024-07-20T06:17:53.234147Z","iopub.status.idle":"2024-07-20T06:17:53.247896Z","shell.execute_reply":"2024-07-20T06:17:53.246980Z","shell.execute_reply.started":"2024-07-20T06:17:53.234484Z"},"trusted":true},"outputs":[],"source":["# Define the path for storing saved model files.\n","saved_models = '/kaggle/working/saved_models'\n","\n","# Check if the directory for saved models exists; if not, create it.\n","# This allows the storage of trained models so to access them without retraining\n","if not os.path.exists(saved_models):\n","        os.makedirs('/kaggle/working/saved_models')"]},{"cell_type":"markdown","metadata":{},"source":["## PreProcessing "]},{"cell_type":"markdown","metadata":{},"source":["The DataPreprocessor class is designed to process trajectory data for agents in a sequence, extracting relevant movement information and preparing it for analysis. It handles loading, filtering, and processing of data to generate motion vectors and masks for both past and future frames.   \n","It is composed of the following methods : \n","- *Initialization* : It constructs the full path to the data file and loads ground truth data from the specified file. It then sets the initial frame and calculates the total number of frames\n","- *Load Ground Truth* : It reads ground truth data from a specified file, expecting tab-limited data. It converts data into a NumPy array, extracts frame numbers, and calculates the range of frames\n","- *Get Id and Get Valid Id* : get_id extracts and returns a list of unique IDs from a given data array. The Ids returend are then passed to the get_valid_id method, which checks IDs in the past and future data frames to ensure they appear in the minimum required number of frames. Only IDs that meet these conditions are considered valid.\n","- *Filter Data by Frame* :  It generates a list of data arrays for frames around a given frame. It can filter for past or future frames based on the past flag. Frame indices are calculated using frame_skip and num_frames parameters \n","- *Compute Motion* : It generates motion vectors and corresponding masks for each valid ID. It processes frames (either past or future) and computes motion by scaling trajectory coordinates\n","- *Call* : It processes a given frame number to compute motion data for both past and future frames. It filters the data to get relevant frames, identifies valid IDs that appear in both past and future data, and calculates the motion vectors and masks for these IDs"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-20T06:17:53.249904Z","iopub.status.busy":"2024-07-20T06:17:53.249504Z","iopub.status.idle":"2024-07-20T06:17:53.285028Z","shell.execute_reply":"2024-07-20T06:17:53.283884Z","shell.execute_reply.started":"2024-07-20T06:17:53.249871Z"},"trusted":true},"outputs":[],"source":["class DataPreprocessor(object):\n","    def __init__(self, data_root, config, seq_name, split, log=None):\n","        '''Initialization of the DataPreprocessor class with essential configurations'''\n","        self.config = config #configuration settings\n","        self.data_root = data_root #root directory for dataset files\n","        self.split = split #indicates the sub-folder to choose (test, train, val)\n","        self.seq_name = seq_name #name of the file to process\n","        self.log = log #logger to debug \n","        self.label_path = os.path.join(data_root, config['dataset'], self.split, seq_name) #construct the full path to the txt file to consider\n","        self.gt = self.load_ground_truth() #load ground truth from the specified label path\n","        self.xind, self.zind = 2, 3  #Index Positions for x and z coordinates in the dataset\n","\n","    def load_ground_truth(self):\n","        '''This function reads the ground truth data from the file located at \"self.label_path\".\n","           It expects data to be tab-limited and attempts to lead it into a numpy array.'''\n","        #load ground truth data from a text file with tab as delimiter\n","        self.gt = np.genfromtxt(self.label_path, delimiter='\\t', dtype=str)\n","        \n","        if self.gt.ndim == 1:\n","            #if data is not loaded as a 2D array, log a warning\n","            print(f\"Warning: The data in {self.label_path} is not loaded as a 2D array.\")\n","            print(f\"Data: {self.gt}\")\n","            \n","        self.gt = self.gt.astype('float32') #convert data type from numerical operations\n","        frames = self.gt[:, 0].astype(np.float32).astype(np.int_)  #extract and convert frame numbers\n","        fr_start, fr_end = frames.min(), frames.max()  #minimum and maximum among all frames\n","        self.init_frame = fr_start  #set the initial frame \n","        self.num_fr = (fr_end + 1 - fr_start)    #get total number of frames\n","        return self.gt\n","        \n","\n","    def get_id(self, data):\n","        \"\"\"This function extracts and returns a list of unique IDs from the given data.\n","           It takes a 2D numpy array `data` as input, extracts the second column \n","           (assumed to be IDs), and returns a copy of this column as a list.\"\"\"\n","        return data[:, 1].copy().tolist()\n","\n","    def filter_data_by_frame(self, frame, frame_skip, num_frames, past=False):\n","        '''Filters data to get subsequent corresponding specific frames\n","           This function generates a list of data arrays for frames around a given frame. \n","           Depending on whether the past flag is set, it will collect frames before or after \n","           the given frame. The frame indices are calculated based on the frame_skip and \n","           num_frames parameters.'''\n","        data_list = [] #initialize empty list to hold the filtered data for each frame\n","        for i in range(num_frames):\n","            #get frame index based on whether we are looking at past or future frames\n","            frame_idx = frame - i * frame_skip if past else frame + (i+1) * frame_skip\n","            \n","            # Check if the calculated frame index is before the initial frame\n","            if frame_idx < self.init_frame:\n","                data = [] # If so, initialize an empty list for data\n","            #filter the ground truth data for the current frame index\n","            data = self.gt[self.gt[:, 0] == frame_idx]\n","            \n","            data_list.append(data) #add filtered data for the current frame to the data list\n","        return data_list #return list of filtered data arrays\n","\n","    def get_valid_id(self, pre_data, fut_data):\n","        '''This function checks the IDs present in the past data frames (`pre_data`) and the future \n","           data frames (`fut_data`). It verifies that each ID appears in a minimum number of past \n","           frames and future frames as specified in the configuration. Only IDs that meet both \n","           conditions are considered valid.'''\n","        cur_id = self.get_id(pre_data[0]) #extract current IDs from the first past data frame\n","        valid_id = [] #initialize an empty list to hold valid IDs\n","        for idx in cur_id:\n","            #check if the ID exists in the required number of past frames\n","            exist_pre = all(idx in data[:, 1] for data in pre_data[:self.config['min_past_frames']] if len(data) > 0)\n","            #check if the ID exists in the required number of future frames\n","            exist_fut = all(idx in data[:, 1] for data in fut_data[:self.config['min_future_frames']] if len(data) > 0)\n","            \n","            #if the ID exists in both past and future frames, add it to the valid_id list\n","            if exist_pre and exist_fut:\n","                valid_id.append(idx)\n","        return valid_id\n","\n","    def compute_motion(self, data_tuple, valid_id, past=True):\n","        '''This function generates motion vectors and corresponding masks for each valid ID \n","        in the data. It processes a series of frames, either past or future, depending on \n","        the 'past' flag, and computes the motion for each ID by scaling the trajectory \n","        coordinates. It handles missing data by carrying forward the last known data.'''\n","        frames = self.config['past_frames'] if past else self.config['future_frames'] #determine the # of frames to process based on \"past\" flag\n","        traj_scale = self.config['traj_scale'] #Trajectory scale factor for normalization\n","        motion = [] #Initialize a list to hold the motion vectors\n","        mask = [] #Initialize a list to hold the masks\n","        \n","        for identity in valid_id:\n","            mask_i = torch.zeros(frames)  #Initialize a tensor for the mask of the current ID\n","            box_3d = torch.zeros([frames, 2])  #Initialize a tensor for the motion vectors of the current ID\n","            for j in range(frames):\n","                data = data_tuple[j]\n","                if len(data) > 0 and identity in data[:, 1]:\n","                    #extract and scale the coordinates for the current ID\n","                    found_data = data[data[:, 1] == identity].squeeze()[[self.xind, self.zind]] / traj_scale\n","                    if past:\n","                        box_3d[frames - 1 - j, :] = torch.from_numpy(found_data).float()\n","                        mask_i[frames - 1 - j] = 1.0\n","                    else:\n","                        box_3d[j, :] = torch.from_numpy(found_data).float()\n","                        mask_i[j] = 1.0\n","                elif j > 0:\n","                    #handle missing data by carrying forward the last known data\n","                    if past:\n","                        box_3d[frames - 1 - j, :] = box_3d[frames - j, :]\n","                    else:\n","                        box_3d[j, :] = box_3d[j - 1, :]\n","                else:\n","                    # Skip the case where the current ID is missing in the first frame\n","                    if past:\n","                        mask_i[frames - 1 - j] = 0.0\n","                        box_3d[frames - 1 - j, :] = torch.zeros(2)\n","                    else:\n","                        mask_i[j] = 0.0\n","                        box_3d[j,:] = torch.zeros(2)\n","            motion.append(box_3d)\n","            mask.append(mask_i)\n","        return motion, mask\n","\n","    def __call__(self, frame):\n","        '''This method processes a given frame number to compute the motion data for both \n","           past and future frames. It filters the data to get relevant frames, identifies \n","           valid IDs that appear in both past and future data, and calculates the motion \n","           vectors and masks for these IDs.'''\n","        #check if the frame is in the valid range\n","        if not (0 <= frame - self.init_frame < self.num_fr):\n","            raise ValueError(f'frame is {frame}, out of range')\n","\n","        pre_data = self.filter_data_by_frame(frame, self.config['frame_skip'], self.config['past_frames'], past=True) #filter data to get past frames\n","        fut_data = self.filter_data_by_frame(frame, self.config['frame_skip'], self.config['future_frames'], past=False) #filter data to get future frames\n","\n","        #identify valid IDs that appear in both past and future frames\n","        valid_id = self.get_valid_id(pre_data, fut_data)\n","        if len(pre_data[0]) == 0 or len(fut_data[0]) == 0 or not valid_id:\n","            #print('None')\n","            return None #if there's no valid ID then return None\n","       \n","        pre_motion_3D, pre_motion_mask = self.compute_motion(pre_data, valid_id, past=True) #compute motion vectors and masks for past frames\n","        fut_motion_3D, fut_motion_mask = self.compute_motion(fut_data, valid_id, past=False) #compute motion vectors and masks for future frames\n","        \n","        #prepare data dictionary with all relevant information\n","        data = {\n","            'pre_motion_3D': pre_motion_3D,\n","            'fut_motion_3D': fut_motion_3D,\n","            'fut_motion_mask': fut_motion_mask,\n","            'pre_motion_mask': pre_motion_mask,\n","            'pre_data': pre_data,\n","            'fut_data': fut_data,\n","            'valid_id': valid_id,\n","            'traj_scale': self.config['traj_scale'],\n","            'seq': self.seq_name,\n","            'frame': frame\n","        }\n","        \n","        #return stuctured data\n","        return data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-20T06:17:53.286618Z","iopub.status.busy":"2024-07-20T06:17:53.286295Z","iopub.status.idle":"2024-07-20T06:17:53.302291Z","shell.execute_reply":"2024-07-20T06:17:53.301099Z","shell.execute_reply.started":"2024-07-20T06:17:53.286591Z"},"trusted":true},"outputs":[],"source":["#seeting up all the necessary parameters to configure the data processing pipeline\n","config = {\n","    'dataset': 'hotel',          # Name of the dataset\n","    'past_frames': 8,          # Number of past frames to consider\n","    'future_frames': 12,       # Number of future frames to consider\n","    'frame_skip': 10,          # Number of frames to skip between each step\n","    'min_past_frames': 8,      # Minimum number of past frames required for a valid ID\n","    'min_future_frames': 12,   # Minimum number of future frames required for a valid ID\n","    'traj_scale': 1,           # Scaling factor for trajectory coordinates\n","    'total_num': 3,            # Total number of frames to process\n","}"]},{"cell_type":"markdown","metadata":{},"source":["## Agent PreProcessing "]},{"cell_type":"markdown","metadata":{},"source":["The AgentPreProcessing class is designed to handle and preprocess trajectory data for agents.\n","We provide two different implementations of the AgentPreProcessing class.  \n","The main difference between the two is represented by the handling of \"invalid entries\".  \n","As already stated in the DataPreprocessor call method, a given frame number is processed to compute motion data for both past and future frames. In doing so, it filters the data to get relevant frames, identifies valid IDs that appear in both past and future data, and calculates the motion vectors and masks for these IDs.  \n","This implies that a lot of data is not processed due to invalid IDs. For this reason, in AgentPreProcessing_with_Invalids we specifically handle this kind of scenario ( see comment in the next section's getitem and reformat_data methods).\n","\n","Except for the getitem, both classes are composed of the same following methods : \n","- *Initialization & Length* : Upon initialization, in the init we set up paths, frame settings, and various parameters. Sequences from the specified directory are read and the total number of samples across all sequences is calculated. Each sequence is processed using the DataPreprocessor class, and the sample indices are adjusted to account for the frame skip interval of 10 frames. A list sample_indeces provides a complete list of indices for all samples, which will be used to access and manage individual samples within the dataset .The length of the dataset is determined by the total number of samples divided by the skip frame interval (10) \n","- *Locating Seq and Frame* : It determines the specific sequence and frame position for a given dataset index by calculating cumulative positions and adjusting for skipped frames \n","- *Processing Data for Varying Agent Counts* : It includes both process_data_for_few_agents and process_data_for_many_agents methods. They handle cases where the number of valid agents in the data differs from the expected total. When there are few agents than expected ones, zero-padded arrays are created for prior and future movements to match the expected agent count. As opposite, if there are more agents, distances are calculated to select the nearest agents, ensuring the data meets the expected count \n","- *Collecting Data* : It collects and stores past and future movement data along with valid agent counts. It concatenates all collected data into single arrays for unified dataset analysis and model training \n"]},{"cell_type":"markdown","metadata":{},"source":["### AgentPreProcessing without Invalids "]},{"cell_type":"markdown","metadata":{},"source":["Specifically for the AgentPreProcessing_without_Invalids : \n","- *Reformat Data* : It reformats the input data into structured arrays for past and future movements. It preprocesses the data, handling cases either fewer or more agents valid agents than the expected total. If data is None, it is skipped and not processed for further analysis and training task \n","- *GetItem* : It is responsible for retrieving and preparing a data sample from the dataset, ensuring that only valid data is returned. It attempts to fetch valid data using a retry mechanism : if the data is invalid, it retries up to a set limit. Once valid data is found, it is reformatted, converted to numpy arrays, and returned. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-20T06:17:53.304617Z","iopub.status.busy":"2024-07-20T06:17:53.304208Z","iopub.status.idle":"2024-07-20T06:17:53.343816Z","shell.execute_reply":"2024-07-20T06:17:53.342706Z","shell.execute_reply.started":"2024-07-20T06:17:53.304580Z"},"trusted":true},"outputs":[],"source":["# trajectory dataset skipping None data\n","class AgentPreProcessing_without_Invalids(Dataset):\n","    def __init__(self, root_path, settings, subset, history_frames, future_frames):\n","        \"\"\"\n","        Initializes the dataset by setting up paths, loading sequences, and processing metadata. \n","        Focusing on sequence Processing, for each sequence in the dataset, the following steps are perfomed :\n","        1. A data processor instance is created; 2. The number of samples is calculated and accumulated, based on frame intervals and sequence length; \n","        3. The sample count and the data processor are stored for each sequence in the dataset ; 4. A list sample_indeces provides a complete list of indices \n","        for all samples, which will be used to access and manage individual samples within the dataset \"\"\"\n","  \n","        self.root_path = root_path\n","        self.settings = settings\n","        self.num_agents = self.settings['total_num']\n","        self.subset = subset\n","        self.directory = os.path.join(self.root_path, self.settings[\"dataset\"], self.subset)\n","        self.sequences = os.listdir(self.directory)\n","        self.history_frames = history_frames\n","        self.future_frames = future_frames\n","        self.minimum_history = history_frames\n","        self.minimum_future = future_frames\n","        self.skip_frames = 10           # Skip frame is set to 10, since frames go 10 by 10 in the provided dataset \n","        self.start_frame = 0\n","        self.total_samples = 0          # Total number of samples, obtained by summing each sequence total number of samples \n","        self.samples_per_sequence = []  # List containing total number of samples per sequence \n","        self.processed_sequences = []   # List containing processed_sequences \n","        self.threshold = 5              # Threshold for distance calculations\n","        self.previous_data = []         # List to store past data    \n","        self.future_data = []           # List to store future data    \n","        self.valid_counts = []          # List to store the count of valid agents     \n","        self.all_prior_data = []        # List to store all prior data \n","        self.all_future_data = []       # List to store all future data \n","        self.all_valid_counts = []      # List to store all valid counts \n","        self.valid_data_count = 0       # Count of valid data samples \n","        self.invalid_data_count = 0     # Count of invalid data samples \n","        \n","        processor_class = DataPreprocessor    # Reference to the data preprocessor class \n","        for sequence_name in self.sequences: \n","            sequence_processor = processor_class(root_path, settings, sequence_name, subset)   # Create a data processor for each sequence \n","            sequence_sample_count = sequence_processor.num_fr + 1 - (self.minimum_history - 1) * self.skip_frames - self.minimum_future * self.skip_frames + 1   # Calculate the number of samples in the sequence \n","            self.total_samples += sequence_sample_count   # Update total samples count \n","            self.samples_per_sequence.append(sequence_sample_count)   # Store the sample count for this sequence \n","            self.processed_sequences.append(sequence_processor)       # Store the processed sequence \n","        \n","        self.sample_indices = list(range(self.total_samples))     # List of sample indices \n","        self.current_index = 0    # Initialize current index \n","        self.samples_per_sequence = [(x + 9) // 10 * 10 for x in self.samples_per_sequence]     # Adjusting samples per sequence for routing to the nearest 10 \n","\n","    def __len__(self):\n","        \"\"\" the length of the dataset is given by the total number of samples divided by 10, since frame skip is 10 \"\"\"\n","        return self.total_samples // 10\n","\n","    def locate_sequence_and_frame(self, index):\n","        \"\"\" locate_sequence_and_frame determines the sequence and the specific frame position within that sequence corresponding to the given dataset index by \n","            calculating the cumulative position and adjusting for skipped frames \"\"\"\n","        current_position = copy.copy(index) * self.skip_frames\n","        for seq_id, count in enumerate(self.samples_per_sequence):\n","            if current_position < count:\n","                frame_position = current_position + (self.minimum_history - 1) * self.skip_frames + self.processed_sequences[seq_id].init_frame\n","                return seq_id, frame_position\n","            current_position -= count\n","        raise ValueError('Index {} is out of range'.format(index))\n","\n","    def reformat_data(self, data):\n","        \"\"\" reformat data reformats the input data into structured arrays for past and future movements. It preprocesses the data, handling cases \n","            either fewer or more agents valid agents than the expected total. If data is None, it is skipped and not processed for further analysis and training \n","            task. The reformatted data is returned \"\"\"\n","        if data is not None:\n","            prior_data, upcoming_data, valids = [], [], []\n","            prior_movement = np.array(torch.stack(data['pre_motion_3D'], dim=0))\n","            future_movement = np.array(torch.stack(data['fut_motion_3D'], dim=0))\n","            agent_count = prior_movement.shape[0]\n","\n","            if agent_count < self.num_agents:\n","                self.process_data_for_few_agents(agent_count, prior_movement, future_movement, prior_data, upcoming_data, valids)\n","            else:\n","                self.process_data_for_many_agents(agent_count, prior_movement, future_movement, prior_data, upcoming_data, valids)\n","\n","        return prior_data, upcoming_data, valids\n","\n","    def process_data_for_few_agents(self, agent_count, prior_movement, future_movement, prior_data, upcoming_data, valids):\n","        \"\"\" process_data_for_few_agents handles cases where the current number of valid agents present in the data is less than the expected total \"\"\"\n","        for i in range(agent_count):\n","            temp = np.zeros((self.num_agents, prior_movement.shape[1], 2))\n","            temp[:agent_count] = prior_movement\n","            prior_data.append(temp[None])\n","\n","            temp = np.zeros((self.num_agents, future_movement.shape[1], 2))\n","            temp[:agent_count] = future_movement\n","            upcoming_data.append(temp[None])\n","            valids.append(agent_count)\n","        self.previous_data = prior_data\n","        self.future_data = upcoming_data\n","        self.valid_counts = valids\n","        \n","\n","    def process_data_for_many_agents(self, agent_count, prior_movement, future_movement, prior_data, upcoming_data, valids):\n","        \"\"\" process_data_for_many_agents handles cases where the current number of valid agents present in the data is greater than or equal to the expected total \"\"\"\n","        for i in range(agent_count):\n","            distances = np.linalg.norm(prior_movement[:, -1] - prior_movement[i:i+1, -1], axis=-1)\n","            close_indices = np.sum((distances < self.threshold).astype(int))\n","\n","            if close_indices < self.num_agents:\n","                temp = np.zeros((self.num_agents, prior_movement.shape[1], 2))\n","                neighbors_idx = np.argsort(distances)\n","                neighbors_idx = neighbors_idx[:close_indices]\n","                temp[:close_indices] = prior_movement[neighbors_idx]\n","                prior_data.append(temp[None])\n","\n","                temp = np.zeros((self.num_agents, future_movement.shape[1], 2))\n","                neighbors_idx = neighbors_idx[:close_indices]\n","                temp[:close_indices] = future_movement[neighbors_idx]\n","                upcoming_data.append(temp[None])\n","                valids.append(close_indices)\n","            else:\n","                neighbors_idx = np.argsort(distances)\n","                assert neighbors_idx[0] == i\n","                neighbors_idx = neighbors_idx[:self.num_agents]\n","                temp = prior_movement[neighbors_idx]\n","                prior_data.append(temp[None])\n","                temp = future_movement[neighbors_idx]\n","                upcoming_data.append(temp[None])\n","                valids.append(self.num_agents)\n","        self.previous_data = prior_data\n","        self.future_data = upcoming_data\n","        self.valid_counts = valids\n","        \n","    \n","    def collect_all_data(self, pre_data, fut_data, num_valid):\n","        \"\"\" collec_att_data collects and stores all past and future movement data, along with the count of valid agents, by appending the provided data to the \n","            corresponding class attributes \"\"\"\n","        self.all_prior_data.append(pre_data)\n","        self.all_future_data.append(fut_data)\n","        self.all_valid_counts.extend(num_valid)\n","    \n","    def get_concatenated_data(self):\n","        \"\"\" get_concated_data concatenates all connected past and future movement dat, along with the count of valid agents, into single arrays. It returns these \n","            concatenated arrays \"\"\"\n","        if self.all_prior_data:\n","            all_past_data = np.concatenate(self.all_prior_data, axis=0)\n","        else:\n","            all_past_data = np.empty((0, self.num_agents, self.history_frames, 2))\n","\n","        if self.all_future_data:\n","            all_future_data = np.concatenate(self.all_future_data, axis=0)\n","        else:\n","            all_future_data = np.empty((0, self.num_agents, self.future_frames, 2))\n","\n","        all_valid_num = np.array(self.all_valid_counts)\n","\n","        return all_past_data, all_future_data, all_valid_num\n","\n","\n","    def __getitem__(self, index):\n","        \"\"\" getitem retrieves a data sample from the dataset. It attempts to fetch valid data using a retry mechanism; \n","        if the data is invalid, it retries up to a set limit. Once valid data is found, it is reformatted, converted to numpy arrays, and returned \"\"\"\n","        retry_count = 0\n","        max_retries = self.total_samples // 10\n","        while retry_count < max_retries:\n","            sample_idx = self.sample_indices[self.current_index]\n","            sequence_id, frame = self.locate_sequence_and_frame(sample_idx)\n","            sequence = self.processed_sequences[sequence_id]\n","            self.current_index += 1\n","\n","            data = sequence(frame)\n","            if data is not None:\n","                self.valid_data_count += 1\n","                break\n","            else:\n","                self.invalid_data_count += 1\n","                if self.valid_data_count == 0 and self.invalid_data_count > 0:\n","                    retry_count += 1\n","                    continue\n","\n","        if retry_count >= max_retries:\n","            raise ValueError(\"Too many invalid data samples after {} retries\".format(max_retries))\n","\n","        prepared_data = self.reformat_data(data)\n","        pre_data, fut_data, num_valid = prepared_data\n","        pre_data = np.array(pre_data, dtype=np.float32)\n","        fut_data = np.array(fut_data, dtype=np.float32)\n","        num_valid = np.array(num_valid)\n","        \n","        pre_data = pre_data.reshape(-1, self.num_agents, self.history_frames, 2)\n","        fut_data = fut_data.reshape(-1, self.num_agents, self.future_frames, 2)\n","        num_valid = num_valid.reshape(-1)\n","\n","        self.collect_all_data(pre_data, fut_data, num_valid)\n","\n","        return pre_data, fut_data, num_valid"]},{"cell_type":"markdown","metadata":{},"source":["### AgentPreProcessing with Invalids "]},{"cell_type":"markdown","metadata":{},"source":["Specifically for the AgentPreProcessing_with_Invalids :  \n","- *Reformatting Data* : It reformats input data into structured arrays for past and future movements. If the data is invalid, it falls back on the last valid data, ensuring consistent input for further processing \n","- *GetItem* : It just retrieves and processes data for a given index by determining the sample index, locating the corresponding sequence and frame, and reformatting the data into prior and future motion data and valid agent counts"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-20T06:17:53.345817Z","iopub.status.busy":"2024-07-20T06:17:53.345452Z","iopub.status.idle":"2024-07-20T06:17:53.383183Z","shell.execute_reply":"2024-07-20T06:17:53.382088Z","shell.execute_reply.started":"2024-07-20T06:17:53.345789Z"},"trusted":true},"outputs":[],"source":["# trajectory dataset handling None data\n","class AgentPreProcessing_with_Invalids(Dataset):\n","    def __init__(self, root_path, settings, subset, history_frames, future_frames):\n","        \n","        self.root_path = root_path\n","        self.settings = settings\n","        self.num_agents = self.settings['total_num']\n","        self.subset = subset\n","        self.directory = os.path.join(self.root_path, self.settings[\"dataset\"], self.subset)\n","        self.sequences = os.listdir(self.directory)\n","        self.history_frames = history_frames\n","        self.future_frames = future_frames\n","        self.minimum_history = history_frames\n","        self.minimum_future = future_frames\n","        self.skip_frames = 10            # Skip frame is set to 10, since frames go 10 by 10 in the provided dataset \n","        self.start_frame = 0\n","        self.total_samples = 0           # Total number of samples, obtained by summing each sequence total number of samples \n","        self.samples_per_sequence = []   # List containing total number of samples per sequence \n","        self.processed_sequences = []    # List containing processed_sequences \n","        self.threshold = 5               # Threshold for distance calculations\n","        self.previous_data = []          # List to store past data    \n","        self.future_data = []            # List to store future data\n","        self.valid_counts = []           # List to store the count of valid agents \n","        self.all_prior_data = []         # List to store all prior data \n","        self.all_future_data = []        # List to store all future data \n","        self.all_valid_counts = []       # List to store all valid counts \n","        self.valid_data_count = 0        # Count of valid data samples \n","        self.invalid_data_count = 0      # Count of invalid data samples \n","        \n","        processor_class = DataPreprocessor    # Reference to the data preprocessor class \n","        for sequence_name in self.sequences:\n","            sequence_processor = processor_class(root_path, settings, sequence_name, subset)     # Create a data processor for each sequence \n","            # calculate the number of samples in the sequence \n","            sequence_sample_count = sequence_processor.num_fr + 1 - (self.minimum_history - 1) * self.skip_frames - self.minimum_future * self.skip_frames + 1\n","            self.total_samples += sequence_sample_count  # Update total samples count\n","            self.samples_per_sequence.append(sequence_sample_count)   # Store the sample count for this sequence \n","            self.processed_sequences.append(sequence_processor)   # Store the procced sequence \n","        \n","        self.sample_indices = list(range(self.total_samples))   # List of sample indices -> [0,1, ..., self.total_samples] \n","        self.current_index = 0   # Initializing current index \n","        self.samples_per_sequence = [(x + 9) // 10 * 10 for x in self.samples_per_sequence]    # Adjusting samples per sequence for routing to the nearest 10 \n","\n","    def __len__(self):\n","        \"\"\" the length of the dataset is given by the total number of samples divided by 10, since frame skip is 10 \"\"\"\n","        return self.total_samples // 10\n","\n","    def locate_sequence_and_frame(self, index):\n","        \"\"\" locate_sequence_and_frame determines the sequence and the specific frame position within that sequence corresponding to the given dataset index by \n","            calculating the cumulative position and adjusting for skipped frames \"\"\"\n","        current_position = copy.copy(index) * self.skip_frames\n","        for seq_id, count in enumerate(self.samples_per_sequence):\n","            if current_position < count:\n","                frame_position = current_position + (self.minimum_history - 1) * self.skip_frames + self.processed_sequences[seq_id].init_frame\n","                return seq_id, frame_position\n","            current_position -= count\n","        raise ValueError('Index {} is out of range'.format(index))\n","\n","    def reformat_data(self, data):\n","        \"\"\" reformat data reformats the input data into structured arrays for past and future movements. It preprocesses the data, handling cases \n","            either fewer or more agents valid agents than the expected total. If the data is invalid, it falls back on the last valid data, ensuring \n","            consistent input for further processing. The reformatted data is returned \"\"\"\n","        if data is not None:\n","            self.valid_data_count += 1\n","            prior_data, upcoming_data, valids = [], [], []\n","            prior_movement = np.array(torch.stack(data['pre_motion_3D'], dim=0))\n","            future_movement = np.array(torch.stack(data['fut_motion_3D'], dim=0))\n","            agent_count = prior_movement.shape[0]\n","\n","            if agent_count < self.num_agents:\n","                self.process_data_for_few_agents(agent_count, prior_movement, future_movement, prior_data, upcoming_data, valids)\n","            else:\n","                self.process_data_for_many_agents(agent_count, prior_movement, future_movement, prior_data, upcoming_data, valids)\n","        else:\n","            # Fallback to previous valid data\n","            prior_data, upcoming_data, valids = self.previous_data, self.future_data, self.valid_counts\n","            self.invalid_data_count += 1\n","\n","        return prior_data, upcoming_data, valids\n","\n","    def process_data_for_few_agents(self, agent_count, prior_movement, future_movement, prior_data, upcoming_data, valids):\n","        \"\"\" process_data_for_few_agents handles cases where the current number of valid agents present in the data is less than the expected total \"\"\"\n","        for i in range(agent_count):\n","            temp = np.zeros((self.num_agents, prior_movement.shape[1], 2))\n","            temp[:agent_count] = prior_movement\n","            prior_data.append(temp[None])\n","\n","            temp = np.zeros((self.num_agents, future_movement.shape[1], 2))\n","            temp[:agent_count] = future_movement\n","            upcoming_data.append(temp[None])\n","            valids.append(agent_count)\n","        self.previous_data = prior_data\n","        self.future_data = upcoming_data\n","        self.valid_counts = valids\n","        \n","\n","    def process_data_for_many_agents(self, agent_count, prior_movement, future_movement, prior_data, upcoming_data, valids):\n","        \"\"\" process_data_for_many_agents handles cases where the current number of valid agents present in the data is greater than or equal to the expected total \"\"\"\n","        for i in range(agent_count):\n","            distances = np.linalg.norm(prior_movement[:, -1] - prior_movement[i:i+1, -1], axis=-1)\n","            close_indices = np.sum((distances < self.threshold).astype(int))\n","\n","            if close_indices < self.num_agents:\n","                temp = np.zeros((self.num_agents, prior_movement.shape[1], 2))\n","                neighbors_idx = np.argsort(distances)\n","                neighbors_idx = neighbors_idx[:close_indices]\n","                temp[:close_indices] = prior_movement[neighbors_idx]\n","                prior_data.append(temp[None])\n","\n","                temp = np.zeros((self.num_agents, future_movement.shape[1], 2))\n","                neighbors_idx = neighbors_idx[:close_indices]\n","                temp[:close_indices] = future_movement[neighbors_idx]\n","                upcoming_data.append(temp[None])\n","                valids.append(close_indices)\n","            else:\n","                neighbors_idx = np.argsort(distances)\n","                assert neighbors_idx[0] == i\n","                neighbors_idx = neighbors_idx[:self.num_agents]\n","                temp = prior_movement[neighbors_idx]\n","                prior_data.append(temp[None])\n","                temp = future_movement[neighbors_idx]\n","                upcoming_data.append(temp[None])\n","                valids.append(self.num_agents)\n","        self.previous_data = prior_data\n","        self.future_data = upcoming_data\n","        self.valid_counts = valids\n","\n","    \n","    def collect_all_data(self, pre_data, fut_data, num_valid):\n","        \"\"\" collec_att_data collects and stores all past and future movement data, along with the count of valid agents, by appending the provided data to the \n","            corresponding class attributes \"\"\"\n","        self.all_prior_data.append(pre_data)\n","        self.all_future_data.append(fut_data)\n","        self.all_valid_counts.extend(num_valid)\n","    \n","    def get_concatenated_data(self):\n","        \"\"\" get_concated_data concatenates all connected past and future movement dat, along with the count of valid agents, into single arrays. It returns these \n","            concatenated arrays \"\"\"\n","        if self.all_prior_data:\n","            all_past_data = np.concatenate(self.all_prior_data, axis=0)\n","        else:\n","            all_past_data = np.empty((0, self.num_agents, self.history_frames, 2))\n","\n","        if self.all_future_data:\n","            all_future_data = np.concatenate(self.all_future_data, axis=0)\n","        else:\n","            all_future_data = np.empty((0, self.num_agents, self.future_frames, 2))\n","\n","        all_valid_num = np.array(self.all_valid_counts)\n","\n","        return all_past_data, all_future_data, all_valid_num\n","\n","\n","    def __getitem__(self, index):\n","        \"\"\" getitem retrieves and process data for a given index. It determines the sample index, using the current index. Then, it locates the corresponding sequence \n","            and frame based on the sample index and retrives the data for the specified frame from the identified sequence. It reformates the retrieved data into prior and future \n","            motion data and valid agent counts \"\"\"\n","        \n","        sample_idx = self.sample_indices[self.current_index]\n","        sequence_id, frame = self.locate_sequence_and_frame(sample_idx)\n","        sequence = self.processed_sequences[sequence_id]\n","        self.current_index += 1\n","\n","        data = sequence(frame)\n","\n","        prepared_data = self.reformat_data(data)\n","        pre_data, fut_data, num_valid = prepared_data\n","        pre_data = np.array(pre_data, dtype=np.float32)\n","        fut_data = np.array(fut_data, dtype=np.float32)\n","        num_valid = np.array(num_valid)\n","        \n","        pre_data = pre_data.reshape(-1, self.num_agents, self.history_frames, 2)\n","        fut_data = fut_data.reshape(-1, self.num_agents, self.future_frames, 2)\n","        num_valid = num_valid.reshape(-1)\n","\n","        self.collect_all_data(pre_data, fut_data, num_valid)\n","\n","        return pre_data, fut_data, num_valid"]},{"cell_type":"markdown","metadata":{},"source":["## Create final dataset"]},{"cell_type":"markdown","metadata":{},"source":["This section focuses on creating the final dataset and defining two classes, TrajectoryDataset_without_Invalids and TrajectoryDataset_with_Invalids, which are used to process and manage trajectory data. These classes are designed to work with the AgentPreProcessing pipeline and handle datasets with and without invalid entries, respectively.\n","\n","The TrajectoryDataset_without_Invalids class processes data excluding any invalid entries, while the TrajectoryDataset_with_Invalids class includes invalid entries in its processing. Both classes convert the trajectory data into a format suitable for model training and evaluation, ensuring that the data is scaled appropriately and organized into past and future sequences."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-20T06:21:47.537199Z","iopub.status.busy":"2024-07-20T06:21:47.535937Z","iopub.status.idle":"2024-07-20T06:21:47.547916Z","shell.execute_reply":"2024-07-20T06:21:47.546647Z","shell.execute_reply.started":"2024-07-20T06:21:47.537161Z"},"trusted":true},"outputs":[],"source":["# Class for Trajectory Dataset without Invalids\n","class TrajectoryDataset_without_Invalids(Dataset):\n","    \"\"\" Defining TrajectoryDataset to use with AgentPreProcessing_without_Invalids \"\"\"\n","    def __init__(self, dataset, valids, settings, history_frames, future_frames):\n","        self.dataset = dataset # Assign the dataset\n","        self.valids = valids # Number of valid data points\n","        \n","        # Process each valid data point in the dataset\n","        for i in range(self.valids):\n","            prior_data, upcoming_data, valid = dataset[i]\n","            \n","        # Concatenate past and future data from the dataset\n","        self.all_past_data, self.all_future_data, self.all_valid_num = dataset.get_concatenated_data()\n","        self.settings = settings # Configuration settings\n","        self.traj_scale = self.settings[\"traj_scale\"] # Scaling factor for trajectory coordinates\n","        self.history_frames = history_frames # Number of past frames\n","        self.future_frames = future_frames # Number of future frames\n","        \n","        # Concatenate past and future data along the third axis and convert to tensor\n","        self.all_past_future = np.concatenate([self.all_past_data, self.all_future_data], axis=2)\n","        self.all_past_future = torch.Tensor(self.all_past_future)\n","        self.all_valid_num = torch.Tensor(self.all_valid_num)\n","        \n","    def __len__(self):\n","        return self.all_past_future.shape[0] # Return the total number of sequences\n","\n","    def __getitem__(self, index):\n","        \"\"\" Returns past_seq, future_seq, number of agents \"\"\"\n","        # Normalize sequence by trajectory scale\n","        seq = self.all_past_future[index] / self.traj_scale\n","        valid_num = self.all_valid_num[index] # Number of valid agents\n","        past_seq = seq[:, :self.history_frames] # Extract past sequence\n","        future_seq = seq[:, self.history_frames:] # Extract future sequence\n","        return past_seq, future_seq, valid_num # Return past sequence, future sequence, and valid number"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-20T06:21:50.556317Z","iopub.status.busy":"2024-07-20T06:21:50.555937Z","iopub.status.idle":"2024-07-20T06:21:50.567029Z","shell.execute_reply":"2024-07-20T06:21:50.565977Z","shell.execute_reply.started":"2024-07-20T06:21:50.556285Z"},"trusted":true},"outputs":[],"source":["# Class for Trajectory Dataset with Invalids\n","class TrajectoryDataset_with_Invalids(Dataset):\n","    \"\"\" Defining TrajectoryDataset to use with AgentPreProcessing_with_Invalids \"\"\"\n","    def __init__(self, dataset, settings, history_frames, future_frames):\n","        self.dataset = dataset # Assign the dataset\n","        \n","        # Process each data point in the dataset\n","        for i in range(len(self.dataset)):\n","            prior_data, upcoming_data, valid = dataset[i]\n","            \n","        # Concatenate past and future data from the dataset\n","        self.all_past_data, self.all_future_data, self.all_valid_num = dataset.get_concatenated_data()\n","        self.settings = settings # Configuration settings\n","        self.traj_scale = self.settings[\"traj_scale\"] # Scaling factor for trajectory coordinates\n","        self.history_frames = history_frames # Number of past frames\n","        self.future_frames = future_frames # Number of future frames\n","        \n","        # Concatenate past and future data along the third axis and convert to tensor\n","        self.all_past_future = np.concatenate([self.all_past_data, self.all_future_data], axis=2)\n","        self.all_past_future = torch.Tensor(self.all_past_future)\n","        self.all_valid_num = torch.Tensor(self.all_valid_num)\n","        \n","    def __len__(self):\n","        return self.all_past_future.shape[0] # Return the total number of sequences\n","\n","    def __getitem__(self, index):\n","        \"\"\" Returns past_seq, future_seq, number of agents \"\"\"\n","        # Normalize sequence by trajectory scale\n","        seq = self.all_past_future[index] / self.traj_scale\n","        valid_num = self.all_valid_num[index] # Number of valid agents\n","        past_seq = seq[:, :self.history_frames] # Extract past sequence\n","        future_seq = seq[:, self.history_frames:] # Extract future sequence\n","        return past_seq, future_seq, valid_num # Return past sequence, future sequence, and valid number"]},{"cell_type":"markdown","metadata":{},"source":["## Defining Options to Train and Test"]},{"cell_type":"markdown","metadata":{},"source":["In this section of the notebook, the script prepares the configurations required to train and test the \"EquiPredict\" model. It defines various parameters, including experiment details, model specifications, and training hyperparameters. These parameters are encapsulated within a dictionary and are further converted into an object for ease of access and manipulation in subsequent code."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-20T06:18:07.406019Z","iopub.status.busy":"2024-07-20T06:18:07.404792Z","iopub.status.idle":"2024-07-20T06:18:07.419369Z","shell.execute_reply":"2024-07-20T06:18:07.418002Z","shell.execute_reply.started":"2024-07-20T06:18:07.405968Z"},"trusted":true},"outputs":[],"source":["# Configuration for the model training and evaluation.\n","# Initialize a dictionary to store all experiment parameters.\n","args = {\n","    'exp_name': 'exp_1',  # Name of the experiment.\n","    'batch_size': 100,  # Number of samples in each batch.\n","    'epochs': 100,  # Total number of training epochs.\n","    'past_length': 8,  # Number of past frames to consider for the prediction.\n","    'future_length': 12,  # Number of future frames to predict.\n","    'no_cuda': False,  # Flag to disable CUDA even if available.\n","    'seed': -1,  # Seed for random number generation. -1 means no specific seed.\n","    'log_interval': 1,  # Frequency of logging training status.\n","    'test_interval': 1,  # Frequency of testing the model.\n","    'outf': 'n_body_system/logs',  # Output directory for logs.\n","    'lr': 1e-6,  # Learning rate for the optimizer.\n","    'epoch_decay': 2,  # Number of epochs after which learning rate will decay.\n","    'lr_gamma': 0.8,  # Learning rate decay factor.\n","    'nf': 64,  # Number of features.\n","    'model': 'egnn_vel',  # Model type to be used.\n","    'attention': 0,  # Whether to use attention mechanism.\n","    'n_layers': 4,  # Number of layers in the neural network.\n","    'degree': 2,  # Degree parameter for some models.\n","    'channels': 64,  # Number of channels in models.\n","    'max_training_samples': 3000,  # Maximum number of training samples to consider.\n","    'dataset': 'nbody',  # Dataset to use.\n","    'sweep_training': 0,  # Whether to use parameter sweeping in training.\n","    'time_exp': 0,  # Flag for time experiment.\n","    'weight_decay': 1e-12,  # Weight decay to prevent overfitting.\n","    'div': 1,  # Division factor for something (ambiguous without context).\n","    'norm_diff': False,  # Whether to normalize differences.\n","    'tanh': False,  # Whether to apply tanh activation.\n","    'subset': 'eth',  # Subset of data to be used.\n","    'model_save_dir': '/kaggle/working/saved_models',  # Directory to save trained models.\n","    'scale': 1,  # Scale factor for inputs/outputs.\n","    'apply_decay': False,  # Whether to apply decay to learning rate.\n","    'res_pred': False,  # Whether to use residual predictions.\n","    'supervise_all': False,  # Whether all layers are supervised.\n","    'model_name': 'eth_ckpt_best',  # Name to save the model checkpoint.\n","    'test_scale': 1,  # Scaling for testing phase.\n","    'test': False,  # Whether to run tests.\n","    'vis': False  # Whether to enable visualization.\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-20T06:18:10.802889Z","iopub.status.busy":"2024-07-20T06:18:10.802464Z","iopub.status.idle":"2024-07-20T06:18:10.834352Z","shell.execute_reply":"2024-07-20T06:18:10.832885Z","shell.execute_reply.started":"2024-07-20T06:18:10.802855Z"},"trusted":true},"outputs":[],"source":["# Create a class to mimic argparse's namespace functionality.\n","class ArgsNamespace:\n","    def __init__(self, adict):\n","        self.__dict__.update(adict)  # Update the object's dictionary with the passed dictionary.\n","\n","# Instantiate the ArgsNamespace class with the args dictionary.\n","args = ArgsNamespace(args)\n","args.cuda = torch.cuda.is_available()  # Check if CUDA is available and update the args object."]},{"cell_type":"markdown","metadata":{},"source":["## Initialize Dataset and Set-up Dataloader - without PreProcessing Invalid Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-20T06:21:56.375995Z","iopub.status.busy":"2024-07-20T06:21:56.375140Z","iopub.status.idle":"2024-07-20T06:22:55.794481Z","shell.execute_reply":"2024-07-20T06:22:55.793526Z","shell.execute_reply.started":"2024-07-20T06:21:56.375963Z"},"trusted":true},"outputs":[],"source":["#''' \n","# Dataset Initialization\n","dataset_train = AgentPreProcessing_without_Invalids(\"/kaggle/input/eth-ucy-processed/datasets/\", config, \"train\", history_frames=args.past_length, future_frames=args.future_length)\n","dataset_val = AgentPreProcessing_without_Invalids(\"/kaggle/input/eth-ucy-processed/datasets/\", config, \"val\", history_frames=args.past_length, future_frames=args.future_length)\n","dataset_test = AgentPreProcessing_without_Invalids(\"/kaggle/input/eth-ucy-processed/datasets/\", config, \"test\", history_frames=args.past_length, future_frames=args.future_length)\n","\n","# final versions of datasets\n","final_dataset_train = TrajectoryDataset_without_Invalids(dataset_train, 3241, config, args.past_length, args.future_length)\n","final_dataset_val = TrajectoryDataset_without_Invalids(dataset_val, 756, config, args.past_length, args.future_length)\n","final_dataset_test = TrajectoryDataset_without_Invalids(dataset_test, 731,  config, args.past_length, args.future_length)\n","\n","# Data Loader Setup\n","loader_train = DataLoader(final_dataset_train, batch_size=args.batch_size, shuffle=True, drop_last=True, num_workers=4)\n","loader_val = DataLoader(final_dataset_val, batch_size=args.batch_size, shuffle=False, drop_last=False, num_workers=4)\n","loader_test = DataLoader(final_dataset_test, batch_size=args.batch_size, shuffle=False, drop_last=False, num_workers=4)\n","#''' "]},{"cell_type":"markdown","metadata":{},"source":["## Initialize Dataset with invalids and Set-up Dataloader - PreProcessing also Invalid Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-19T09:51:10.545542Z","iopub.status.busy":"2024-07-19T09:51:10.545138Z","iopub.status.idle":"2024-07-19T09:51:10.553203Z","shell.execute_reply":"2024-07-19T09:51:10.552314Z","shell.execute_reply.started":"2024-07-19T09:51:10.545505Z"},"trusted":true},"outputs":[],"source":["'''\n","# Dataset Initialization\n","dataset_train = AgentPreProcessing_with_Invalids(\"/kaggle/input/eth-ucy-processed/datasets/\", config, \"train\", history_frames=args.past_length, future_frames=args.future_length)\n","dataset_val = AgentPreProcessing_with_Invalids(\"/kaggle/input/eth-ucy-processed/datasets/\", config, \"val\", history_frames=args.past_length, future_frames=args.future_length)\n","dataset_test = AgentPreProcessing_with_Invalids(\"/kaggle/input/eth-ucy-processed/datasets/\", config, \"test\", history_frames=args.past_length, future_frames=args.future_length)\n","\n","# final versions of datasets\n","final_dataset_train = TrajectoryDataset_with_Invalids(dataset_train, config, args.past_length, args.future_length)\n","final_dataset_val = TrajectoryDataset_with_Invalids(dataset_val, config, args.past_length, args.future_length)\n","final_dataset_test = TrajectoryDataset_with_Invalids(dataset_test, config, args.past_length, args.future_length)\n","\n","# Data Loader Setup\n","loader_train = DataLoader(final_dataset_train, batch_size=args.batch_size, shuffle=True, drop_last=True, num_workers=4)\n","loader_val = DataLoader(final_dataset_val, batch_size=args.batch_size, shuffle=False, drop_last=False, num_workers=4)\n","loader_test = DataLoader(final_dataset_test, batch_size=args.batch_size, shuffle=False, drop_last=False, num_workers=4)\n","'''"]},{"cell_type":"markdown","metadata":{},"source":["## Definition of all model components\n","\n","This code defines several classes related to the neural network architecture for a machine learning model that deals with motion or trajectory prediction. The model components include feature initialization, interaction graphs, and a feature learning layer, each encapsulated within separate classes. These classes handle different aspects of feature processing and interaction reasoning among agents within the system, crucial for accurately predicting trajectories based on past and current data inputs.\n","\n","#### **1. Feature Initialization**\n","\n","The class is designed to initialize and transform input features into a more suitable format for subsequent processing by other parts of a neural network model. This transformation is critical for extracting and enhancing underlying patterns in the data that are not immediately apparent in their raw form.\n","It includes:\n","- *Embedding Layers*: Two separate layers transform the input features and velocity angles into a hidden space.\n","- *Activation Function*: A ReLU function introduces non-linearity, helping to capture more complex patterns.\n","- *Weight Initialization*: Uses Xavier initialization to optimize training by maintaining stable gradients.\n","- *Forward Pass*: Combines transformed input and velocity features into a single feature vector for subsequent processing."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-19T09:51:10.554539Z","iopub.status.busy":"2024-07-19T09:51:10.554210Z","iopub.status.idle":"2024-07-19T09:51:10.568272Z","shell.execute_reply":"2024-07-19T09:51:10.567426Z","shell.execute_reply.started":"2024-07-19T09:51:10.554514Z"},"trusted":true},"outputs":[],"source":["class FeatureInitialization(nn.Module):\n","    \"\"\"Class for initializing features of input data\"\"\"\n","    def __init__(self, in_node_nf, hidden_nf, act_fn=nn.ReLU):\n","        super().__init__()\n","        # Embedding layers to transform input node features into a hidden feature space.\n","        self.embedding = nn.Linear(in_node_nf, hidden_nf // 2)\n","        self.embedding2 = nn.Linear(in_node_nf, hidden_nf // 2)\n","        \n","        self.act_fn = act_fn() # Activation function for non-linearity in feature transformation.\n","        self._initialize_weights() # Initialize weights with a specific strategy for better training performance.\n","\n","    def initialize_weights(self):\n","        \"\"\"Method to initialize weights of embedding layers using Xavier initialization\"\"\"\n","        init.xavier_uniform_(self.embedding.weight)\n","        init.xavier_uniform_(self.embedding2.weight)\n","\n","    def forward(self, h, vel_angle):\n","        \"\"\"Forward pass to compute the combined feature vector from input features and velocity angles\"\"\"\n","        h = self.embedding(h)\n","        vel_angle_embedding = self.embedding2(vel_angle)\n","        return torch.cat([h, vel_angle_embedding], dim=-1)"]},{"cell_type":"markdown","metadata":{},"source":["#### **2. Interaction Graph**\n","\n","This class models the interactions among agents by processing and combining their feature vectors through several multilayer perceptrons (MLPs). It also categorizes these interactions into different groups, which can help in segmenting agent behaviors or predicting different interaction outcomes.\n","\n","It's structured in 2 main parts:\n","- *Initialization*:\n","    - Configuration Parameters: Accepts parameters for the sizes of hidden layers (hidden_nf), channel sizes (hid_channel), activation function (act_fn), and the number of categories (category_num).\n","      MLPs for Edge, Coordinate, and Node Features:\n","    - edge_mlp: Processes combined features of pairs of agents to learn about their direct interactions.\n","    - coord_mlp: Transforms coordinate features into a space that enhances their representation.\n","    - node_mlp: Processes node features to refine their information after interactions are considered.\n","    - category_mlp: Determines the category of interactions among agents based on their features.\n","    \n","- *Category Calculation*:\n","    - Feature Preparation: Flattens and concatenates the features of all agents for clustering.\n","    - Clustering: Uses K-means to categorize the interactions into predefined groups based on feature similarity.\n","    - Output Formatting: Transforms clustering labels into one-hot encoded format and adjusts them to match the expected dimensions for further processing in the network."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-19T09:51:10.569831Z","iopub.status.busy":"2024-07-19T09:51:10.569527Z","iopub.status.idle":"2024-07-19T09:51:10.582111Z","shell.execute_reply":"2024-07-19T09:51:10.581378Z","shell.execute_reply.started":"2024-07-19T09:51:10.569806Z"},"trusted":true},"outputs":[],"source":["class InteractionGraph(nn.Module):\n","    \"\"\"Class to model interaction graphs between agents using learned features\"\"\"\n","    def __init__(self, hidden_nf, hid_channel, act_fn=nn.ReLU, category_num=4):\n","        super().__init__()\n","        self.hidden_nf = hidden_nf  # Number of features in hidden layers.\n","        self.hid_channel = hid_channel  # Number of channels in hidden layers.\n","        self.category_num = category_num  # Number of categories to classify interactions.\n","        self.tao = 1  # Parameter, possibly a threshold or scaling factor, to be defined.\n","\n","        # Define MLPs for processing different aspects of agent interaction.\n","        self.edge_mlp = nn.Sequential(\n","            nn.Linear(hidden_nf * 2 + hid_channel * 2, hidden_nf),\n","            act_fn(),\n","            nn.Linear(hidden_nf, hidden_nf),\n","            act_fn()\n","        )\n","\n","        self.coord_mlp = nn.Sequential(\n","            nn.Linear(hid_channel, hidden_nf),\n","            act_fn(),\n","            nn.Linear(hidden_nf, hid_channel * 2),\n","            act_fn()\n","        )\n","\n","        self.node_mlp = nn.Sequential(\n","            nn.Linear(hidden_nf + hidden_nf, hidden_nf),\n","            act_fn(),\n","            nn.Linear(hidden_nf, hidden_nf)\n","        )\n","\n","        self.category_mlp = nn.Sequential(\n","            nn.Linear(hidden_nf * 2 + hid_channel * 2, hidden_nf),\n","            act_fn(),\n","            nn.Linear(hidden_nf, category_num),\n","            act_fn()\n","        )\n","\n","    def calc_category(self, h, coord, valid_mask):\n","        \"\"\"Method to calculate interaction categories based on combined features using clustering\"\"\"\n","        batch_size, agent_num = coord.shape[:2]\n","        \n","        # Flatten and concatenate features for clustering\n","        h_flat = h.view(batch_size * agent_num, -1)\n","        coord_flat = coord.view(batch_size * agent_num, -1)\n","        features = torch.cat([h_flat, coord_flat], dim=-1).detach().cpu().numpy()\n","        \n","        # Perform K-means clustering to categorize interactions\n","        kmeans = KMeans(n_clusters=self.category_num)\n","        kmeans.fit(features)\n","        cluster_labels = kmeans.labels_\n","        \n","        # Reshape cluster labels to match the batch and agent dimensions\n","        cluster_labels = torch.tensor(cluster_labels, dtype=torch.long, device=h.device)\n","        cluster_labels = cluster_labels.view(batch_size, agent_num)\n","        \n","        # Create one-hot encoding for cluster labels\n","        interaction_category = F.one_hot(cluster_labels, num_classes=self.category_num).float()\n","        \n","        # Expand dimensions to match expected output shape\n","        interaction_category = interaction_category.unsqueeze(2).expand(-1, -1, agent_num, -1)\n","        \n","        return interaction_category"]},{"cell_type":"markdown","metadata":{},"source":["#### **3. Feature Learning Layer**\n","\n","This class is designed to process the features of agents, update their state based on interactions, and apply attention mechanisms to focus on relevant parts of the data dynamically. It uses multiple neural network layers to compute and refine feature representations, which are critical for tasks like trajectory prediction.\n","\n","It's organized into 4 main parts:\n","- *Initialization*:\n","    - Configuration Parameters: Takes multiple parameters defining the sizes of inputs, outputs, and features, as well as flags for using recurrent updates, attention mechanisms, and reasoning.\n","    - Neural Network Layers:\n","        - coord_vel: Linear layer to update velocities.\n","        - edge_mlp, node_mlp, category_mlps: Multiple MLPs to process edge, node, and categorical features.\n","        - attention components (query, key): Defined if attention is enabled, for calculating attention weights.\n","- *MLP Builder*: Constructs MLPs dynamically based on specified input and output sizes and includes nonlinear activation functions, which can be extended with tanh activations if specified.\n","- *Feature Computation Methods*:\n","    - compute_edge_features: Calculates features based on the difference between agent coordinates.\n","    - update_coordinates: Updates agent coordinates based on edge features.\n","    - compute_node_features: Refines node features by incorporating aggregated edge information.\n","    - apply_attention: Adjusts coordinates based on attention-weighted features.\n","- *Forward Pass*: Integrates all the computations to update agent states and apply attention, returning updated feature representations and coordinates."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-19T09:51:10.583424Z","iopub.status.busy":"2024-07-19T09:51:10.583145Z","iopub.status.idle":"2024-07-19T09:51:10.605193Z","shell.execute_reply":"2024-07-19T09:51:10.604431Z","shell.execute_reply.started":"2024-07-19T09:51:10.583400Z"},"trusted":true},"outputs":[],"source":["class Feature_learning_layer(nn.Module):\n","    \"\"\"Class for learning and updating features in an agent-based model\"\"\"\n","    def __init__(self, input_nf, output_nf, hidden_nf, input_c, hidden_c, output_c, edges_in_d=0, nodes_att_dim=0, act_fn=nn.ReLU, recurrent=True, coords_weight=1.0, attention=True, norm_diff=False, tanh=False, apply_reasoning=True, input_reasoning=False, category_num=2):\n","        super().__init__()\n","        # Flags for model behavior and feature processing configurations.\n","        self.recurrent = recurrent  # Determines if layer updates should be recurrent.\n","        self.attention = attention  # Determines if attention mechanisms should be applied.\n","        self.apply_reasoning = apply_reasoning  # Enables reasoning within the layer.\n","        self.category_num = category_num  # Number of output categories for classification tasks.\n","\n","        # Core layers for feature processing.\n","        self.coord_vel = nn.Linear(2, 2, bias=False)  # Layer to update velocities.\n","        self.edge_mlp = self._build_mlp(input_nf * 2 + edges_in_d + hidden_c, hidden_nf, act_fn, layers=3)  # MLP for processing edge features.\n","        self.node_mlp = self._build_mlp(input_nf + hidden_nf + nodes_att_dim, output_nf, act_fn)  # MLP for node feature aggregation.\n","        self.category_mlps = nn.ModuleList([self._build_mlp(input_nf * 2 + hidden_c, hidden_c, act_fn, layers=3) for _ in range(category_num)])  # List of MLPs for different categories.\n","        \n","        # Attention mechanisms, instantiated if enabled.\n","        if attention:\n","            self.query = nn.Linear(hidden_c, hidden_c, bias=False)  # Generates query vectors for the attention mechanism.\n","            self.key = nn.Linear(hidden_c, hidden_c, bias=False)  # Generates key vectors for the attention mechanism.\n","\n","        self.inner_attention_mlp = nn.Sequential(nn.Linear(hidden_nf, hidden_c), act_fn())  # MLP to process features within the attention mechanism.\n","\n","    def build_mlp(self, input_size, output_size, act_fn, layers=2, add_tanh=False):\n","        \"\"\"Function to dynamically build MLP structures\"\"\"\n","        mlp_layers = [nn.Linear(input_size, output_size), act_fn()]\n","        for _ in range(1, layers):\n","            mlp_layers.append(nn.Linear(output_size, output_size))\n","            mlp_layers.append(act_fn())\n","        if add_tanh:\n","            mlp_layers.append(nn.Tanh())  # Adds an optional Tanh layer for additional non-linearity.\n","        return nn.Sequential(*mlp_layers)\n","\n","    def compute_edge_features(self, h, coord):\n","        \"\"\"Computes edge features by considering spatial relationships and feature differences\"\"\"\n","        batch_size, agent_num, coord_dim, _ = coord.shape\n","        h1 = h.unsqueeze(2).expand(-1, -1, agent_num, -1)  # Expand features for each agent pair.\n","        h2 = h.unsqueeze(1).expand(-1, agent_num, -1, -1)  # Second expansion for pairwise comparison.\n","        coord_diff = coord.unsqueeze(2) - coord.unsqueeze(1)  # Compute pairwise coordinate differences.\n","        coord_dist = coord_diff.norm(dim=-1, keepdim=False)  # Calculate Euclidean distance between coordinates.\n","        edge_features = torch.cat((h1, h2, coord_dist), dim=-1)  # Concatenate features and distances.\n","        return self.edge_mlp(edge_features)  # Process concatenated features through an MLP.\n","\n","    def update_coordinates(self, coord, edge_features):\n","        \"\"\"Update agent coordinates based on computed edge features\"\"\"\n","        coord_factors = edge_features.mean(dim=2)  # Average features across dimensions.\n","        coord_factors = coord_factors.unsqueeze(-1).expand_as(coord)  # Expand features to match coordinate dimensions.\n","        return coord + coord_factors  # Update coordinates by adding feature-driven adjustments.\n","\n","    def compute_node_features(self, h, edge_features, valid_mask):\n","        \"\"\"Aggregate and refine node features using the edge features\"\"\"\n","        aggregated_edges = edge_features.sum(dim=2)  # Sum edge features to aggregate information.\n","        return self.node_mlp(torch.cat((h, aggregated_edges), dim=-1))  # Combine and process node and edge features.\n","\n","    def apply_attention(self, coord, h, valid_mask_agent, num_valid):\n","        \"\"\"Apply attention to refine feature adjustments based on their relevance\"\"\"\n","        query = self.query(h)  # Generate query vectors.\n","        key = self.key(h).transpose(1, 2)  # Generate and transpose key vectors.\n","        att_weights = torch.bmm(query, key)  # Compute raw attention weights.\n","        seq_len = valid_mask_agent.size(1)\n","        valid_mask_agent = valid_mask_agent.squeeze(-1).transpose(1, 2).expand(-1, seq_len, -1)  # Adjust and expand valid masks.\n","        att_weights = F.softmax(att_weights, dim=2) * valid_mask_agent  # Apply softmax and mask to normalize attention weights.\n","        coord_flattened = coord.reshape(coord.size(0), coord.size(1), -1)  # Flatten coordinates for matrix multiplication.\n","        coord_adjusted = torch.bmm(att_weights, coord_flattened)  # Adjust coordinates based on attention weights.\n","        coord_final = coord_adjusted.reshape(coord.size(0), coord.size(1), coord.size(2), coord.size(3))  # Reshape back to original dimensions.\n","        return coord_final  # Return adjusted coordinates.\n","\n","    def forward(self, h, coord, vel, valid_mask, valid_mask_agent, num_valid, category=None):\n","        \"\"\"Forward pass integrates all computations to update agent features and coordinates based on interactions and attention\"\"\"\n","        edge_features = self.compute_edge_features(h, coord)  # Compute edge features.\n","        coord = self.update_coordinates(coord, edge_features)  # Update coordinates based on edge features.\n","        coord += self.coord_vel(vel)  # Apply velocity updates.\n","        h = self.compute_node_features(h, edge_features, valid_mask)  # Compute and update node features.\n","        coord = self.apply_attention(coord, h, valid_mask_agent, num_valid)  # Refine coordinates using attention.\n","        return h, coord, category  # Return updated features and coordinates."]},{"cell_type":"markdown","metadata":{},"source":["### EquiPredict "]},{"cell_type":"markdown","metadata":{},"source":["EquiPredict is a neural network model designed for motion prediction in a graph-based or relational context. It integrates multiple advanced neural network components, including multi-head attention, graph convolutional layers (GCNs / GATs), and recurrent layers (LSTMs).  \n","It is used to predict the future motion of nodes (agents) in a dynamic system based on their current features, positions, and velocities.   \n","Looking deeply at EqMotion forward function, the following steps are perfomed :  \n","- 1. **Embedding and Transforming Features** : \n","        - *Node features and Velocity Angles* : They are transformed into lower-dimensional spaces to capture essential characteristics while reducing dimensionality and then embedded. This allows the model to handle and interpret complex features associaed with the agents' state and movements. \n","        - *Geometric Coordinates and Velocities* : Transformations are then applied to the geometric coordinates and velocities of the agents, so they are scaled and normalized appropriately. By transforming coordinates and velocities into a higher-dimensional space, the model enhances its ability to capture subtle patterns and interactions.\n","- 2. **Discrete Cosine Transform** : Data is transformed into the frequency domain to simplify representation and capture nuanced patterns. Inverse DCT converts data back to the original domain for accurate predictions. IDCT will be used to convert back the final output \n","- 3. **Interaction Categories and Feature Learning Layers** : \n","This step is essential for understanding how agents interact with one another based on their features and coordinates. \n","        - *Interaction categories* : They are computed using message passing and aggregation techniques, which involve evaluating how different agents influence each other. These categories guide the model in learning meaningful interactions between agents, helping refine their representations. \n","        - *Feature Learning Layers and Graph Convolutional Layer* : The model utilizies Feature Learning Layers to process these interactions. By applying those layers, the model updates the node features based on the computed interaction categories, effectively capturing the relationships and dependencies among agents.\n","- 4. **Recurrent Processing for Temporal Dynamics** : If recurrent processing is enabled, the model incorporates an LSTM layer. It processes the node features across time steps, allowing the model to account for how agent interactions and states evolve over time. \n","- 5. **Final Predictions** : The final component of the forward pass involves generating predictions using multiple prediction heads. Each head processes the refined node features to produce outputs related to the coordinates of the agents. The use of multiple prediction heads allows the model to generate diverse predictions and aggregate them for improved accuracy. After predictions are made, if DCT was applied earlier, the model transforms the coordinates back from the frequency domain to the spatial domain using the inverse DCT. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-19T09:51:10.606601Z","iopub.status.busy":"2024-07-19T09:51:10.606335Z","iopub.status.idle":"2024-07-19T09:51:10.620045Z","shell.execute_reply":"2024-07-19T09:51:10.619103Z","shell.execute_reply.started":"2024-07-19T09:51:10.606578Z"},"trusted":true},"outputs":[],"source":["\"\"\" MultiHead Attention processes input sequences through multiple parallel attention heads, each learning different  \n","aspects of the relationships between tokens. The results from each head are combined and projected back to the desired \n","output dimension. This approach helps the model capture diverse features and dependencies in the input data \"\"\"\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, device, num_heads=4):\n","        super(MultiHeadAttention, self).__init__()\n","        self.device = device\n","        \n","        self.num_heads = num_heads\n","        self.hidden_dim = hidden_dim\n","        self.head_dim = hidden_dim // num_heads\n","        \n","        self.linear_layers = nn.ModuleList([\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.Linear(hidden_dim, hidden_dim)\n","        ])\n","        \n","        # Move all linear layers to device\n","        self.linear_layers.to(self.device)\n","        self.to(self.device)\n","\n","    def forward(self, query, key, value, mask=None):\n","    \n","        batch_size, seq_length, _, _ = query.shape        \n","        \n","        # Move inputs to device\n","        query = query.to(self.device)\n","        key = key.to(self.device)\n","        value = value.to(self.device)\n","        \n","        # Project inputs using ModuleList\n","        Q = self.linear_layers[0](query)\n","        K = self.linear_layers[1](key)\n","        V = self.linear_layers[2](value)\n","        \n","        # Split heads\n","        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n","        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n","        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n","        \n","        # Calculate attention scores\n","        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n","        if mask is not None:\n","            attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n","        \n","        attention_weights = F.softmax(attention_scores, dim=-1)\n","        attention_output = torch.matmul(attention_weights, V)\n","        \n","        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, -1, self.hidden_dim)\n","        attention_output = self.linear_layers[3](attention_output)\n","        attention_output = attention_output.view(batch_size, seq_length, seq_length, attention_output.size(2))\n","        \n","        return attention_output"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-19T09:51:10.645042Z","iopub.status.busy":"2024-07-19T09:51:10.644406Z","iopub.status.idle":"2024-07-19T09:51:10.696141Z","shell.execute_reply":"2024-07-19T09:51:10.695308Z","shell.execute_reply.started":"2024-07-19T09:51:10.645011Z"},"trusted":true},"outputs":[],"source":["\n","class EquiPredict(nn.Module):\n","    def __init__(self, node_features, edge_features, hidden_dim, input_dim, hidden_channel_dim, output_dim, device='cuda', act_fn=nn.SiLU(), layers=4, coords_weight=1.0, use_recurrent=False, normalize_diff=False, use_tanh=False, gnn_variant = 'GCN'):\n","        super(EquiPredict, self).__init__()\n","        self.hidden_dim = hidden_dim\n","        self.device = device\n","        self.layers = layers\n","        self.device = device \n","        self.use_recurrent = use_recurrent\n","\n","        self.node_embedding = nn.Linear(node_features, hidden_dim // 2)\n","        self.angle_embedding = nn.Linear(node_features, hidden_dim // 2)\n","\n","        self.coord_transform = nn.Linear(input_dim, hidden_channel_dim, bias=False)\n","        self.velocity_transform = nn.Linear(input_dim, hidden_channel_dim, bias=False)\n","\n","        self.use_dct = True\n","        self.validate_reasoning = True\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","\n","        self.num_categories = 4\n","        self.tao = 1\n","\n","        self.given_category = False\n","        if not self.given_category:\n","            self.edge_network, self.coord_network, self.node_network, self.category_network = self.init_mlps(hidden_dim, hidden_channel_dim, act_fn)\n","        \n","        \n","        # Choose GCN VARIANT \n","        self.gnn_variant = gnn_variant \n","        if self.gnn_variant == 'GCN':\n","            self.gcl = GCNConv(self.hidden_dim, self.hidden_dim)\n","        elif self.gnn_variant == 'GAT':\n","            self.gcl = GATConv(self.hidden_dim, self.hidden_dim)        \n","        \n","        # Feature Learning Layers\n","        self.gcls = nn.ModuleList([self.create_gcl_layer(edge_features, hidden_dim, input_dim, hidden_channel_dim, output_dim, act_fn = nn.SiLU(), coords_weight = 1.0, recurrent = False, norm_diff = False, tanh = False) for _ in range(layers - 1)])\n","\n","        # Prediction Heads\n","        self.predict_heads = nn.ModuleList([self.create_gcl_layer(edge_features, hidden_dim, input_dim, hidden_channel_dim, output_dim, act_fn = nn.SiLU(), coords_weight = 1.0, recurrent = False , norm_diff = False, tanh = False) for _ in range(20)])\n","        self.predict_heads_linear = nn.ModuleList([nn.Linear(hidden_channel_dim, output_dim, bias=False) for _ in range(20)])\n","\n","        self.to(self.device)\n","    \n","    def init_mlps(self, hidden_dim, hidden_channel_dim, act_fn):\n","        \"\"\" init_mlp defined mlps that will be used later \"\"\"\n","        edge_network = nn.Sequential(\n","            nn.Linear(hidden_dim * 2 + hidden_channel_dim * 2, hidden_dim),\n","            act_fn,\n","            nn.Linear(hidden_dim, hidden_dim),\n","            act_fn\n","        )\n","\n","        coord_network = nn.Sequential(\n","            nn.Linear(hidden_channel_dim * 2, hidden_dim),\n","            act_fn,\n","            nn.Linear(hidden_dim, hidden_channel_dim * 2),\n","            act_fn\n","        )\n","\n","        node_network = nn.Sequential(\n","            nn.Linear(hidden_dim * 2, hidden_dim),\n","            act_fn,\n","            nn.Linear(hidden_dim, hidden_dim),\n","            act_fn\n","        )\n","\n","        category_network = nn.Sequential(\n","            nn.Linear(hidden_dim * 2 + hidden_channel_dim * 2, hidden_dim),\n","            act_fn,\n","            nn.Linear(hidden_dim, self.num_categories),\n","            act_fn\n","        )\n","\n","        return edge_network, coord_network, node_network, category_network\n","    \n","    def create_gcl_layer(self, in_edge_nf, hidden_nf, in_channel, hid_channel, out_channel, act_fn, coords_weight, recurrent, norm_diff, tanh):\n","        return Feature_learning_layer(hidden_nf, hidden_nf, hidden_nf, in_channel, hid_channel, out_channel, edges_in_d=0, nodes_att_dim=0, act_fn=nn.ReLU, recurrent=True, coords_weight=1.0, attention=True, norm_diff=False, tanh=False, apply_reasoning=False, input_reasoning=True, category_num=self.num_categories)\n","    \n","    def compute_dct_matrix(self, N, x):\n","        \"\"\" compute_dct_matrix compute the Discrete Cosine Transform (DCT) matrix and its inverse (IDCT). The DCT matrix is used to transform data into the frequency domain, while the IDCT matrix is used\n","            to transform data back from the frequency domain to the spatial domain\"\"\"\n","        dct_matrix = np.eye(N)\n","        for k in range(N):\n","            for i in range(N):\n","                weight = np.sqrt(2 / N)\n","                if k == 0:\n","                    weight = np.sqrt(1 / N)\n","                dct_matrix[k, i] = weight * np.cos(np.pi * (i + 0.5) * k / N)\n","        idct_matrix = np.linalg.inv(dct_matrix)\n","        dct_matrix = torch.from_numpy(dct_matrix).type_as(x)\n","        idct_matrix = torch.from_numpy(idct_matrix).type_as(x)\n","        return dct_matrix, idct_matrix\n","    \n","    def apply_dct(self, coords, vel, valid_agent_mask, agent_num, num_valid, batch_size):\n","        \"\"\" apply_dct applies Discrete Cosine Transform (DCT) to coordinates and velocities\"\"\"\n","        coords_center = torch.mean(coords * valid_agent_mask, dim=(1, 2), keepdim=True) * (agent_num / num_valid[:, None, None, None])\n","        coords -= coords_center\n","        dct_m, idct_m = self.compute_dct_matrix(self.input_dim, coords), self.compute_dct_matrix(self.output_dim, coords)\n","        dct_m, idct_m = dct_m[0].repeat(batch_size, agent_num, 1, 1), idct_m[1].repeat(batch_size, agent_num, 1, 1)\n","        coords, vel = torch.matmul(dct_m, coords), torch.matmul(dct_m, vel)\n","        return coords, coords_center, vel, idct_m\n","    \n","\n","    def compute_interaction_categories(self, node_features, coords, valid_mask):\n","        \"\"\" compute_interaction_categories computes interaction categories between nodes based on their features and coordinates \"\"\"\n","        \n","        batch_size, num_agents, _, _ = coords.shape\n","        node_features_1 = node_features[:, :, None, :].repeat(1,1,num_agents,1)\n","        node_features_2 = node_features[:, None, :, :].repeat(1,num_agents,1,1)\n","        \n","        # Calculate coordinate differences and distances\n","        coord_diff = coords[:, :, None, :, :] - coords[:, None, :, :, :]\n","        distances = torch.norm(coord_diff, dim=-1)\n","        distances = self.coord_network(distances)\n","        \n","        # Initialize edge features\n","        edge_features = self.message_passing(node_features_1, node_features_2, distances)\n","\n","        # Compute interaction categories through message passing\n","        interaction_categories = self.message_aggregation(node_features, edge_features, distances, valid_mask, num_agents, batch_size)\n","\n","        return interaction_categories\n","\n","    def message_passing(self, node_features_1, node_features_2, distances):\n","        \"\"\" message_passing performs message passing to compute edge features using multi-head attention \"\"\"\n","    \n","        edge_input = torch.cat([node_features_1, node_features_2, distances], dim=-1)\n","        \n","        # Apply multi-head attention\n","        multihead_attention = MultiHeadAttention(input_dim=edge_input.size(-1), hidden_dim=self.hidden_dim, device = self.device)\n","        edge_features = multihead_attention(edge_input, edge_input, edge_input)  # Self-attention\n","        \n","\n","        return edge_features\n","\n","    def message_aggregation(self, node_features, computed_edge_features, distances, valid_mask, num_agents, batch_size):\n","        \"\"\" message_aggregation aggregates edge features to update node representations and compute interaction categories \"\"\"\n","        # Prepare mask to ignore self-loops\n","        mask = (torch.ones((num_agents, num_agents)) - torch.eye(num_agents)).type_as(computed_edge_features)\n","        mask = mask[None, :, :, None].repeat(batch_size, 1, 1, 1)\n","\n","        # Aggregate edge features and update node representations\n","        updated_node_features = self.node_network(torch.cat([node_features, torch.sum(valid_mask * mask * computed_edge_features, dim=2)], dim=-1))\n","\n","        # Prepare updated node features for interaction computation\n","        updated_node_features_1 = updated_node_features[:, :, None, :].repeat(1,1, num_agents,1)\n","        updated_node_features_2 = updated_node_features[:, None, :, :].repeat(1, num_agents, 1,1)\n","        updated_edge_input = torch.cat([updated_node_features_1, updated_node_features_2, distances], dim=-1)\n","\n","        # Compute interaction categories\n","        interaction_categories = F.softmax(self.category_network(updated_edge_input) / self.tao, dim=-1)\n","\n","        return interaction_categories\n","\n","    def create_valid_mask(self, num_valid, num_agents):\n","        \"\"\" create_valid_mask create a mask to indicate valid interactions between agents in a 2D grid \"\"\"\n","        batch_size = num_valid.shape[0]\n","        valid_mask = torch.zeros((batch_size, num_agents, num_agents))\n","        for i in range(batch_size):\n","            valid_mask[i, :num_valid[i], :num_valid[i]] = 1\n","        return valid_mask.unsqueeze(-1)\n","\n","    def create_valid_mask2(self, num_valid, num_agents):\n","        \"\"\" create_valid_mask2 creates a mask to indicate valid agents in a 1D vector.\"\"\"\n","        batch_size = num_valid.shape[0]\n","        valid_mask = torch.zeros((batch_size, num_agents))\n","        for i in range(batch_size):\n","            valid_mask[i, :num_valid[i]] = 1\n","        return valid_mask.unsqueeze(-1).unsqueeze(-1)\n","\n","    def forward(self, node_features, coords, velocities, num_valid, edge_attr=None):\n","        \"\"\" forward method is the core of the EqMotion model, explained in details above \"\"\"\n","        \n","        # Defining previous velocities, used to compute the cosine of the angle between them and the current velocity vectors \n","        velocities_pre = torch.zeros_like(velocities)\n","        velocities_pre[:, :, 1:] = velocities[:, :, :-1]\n","        velocities_pre[:, :, 0] = velocities[:, :, 0]\n","        EPS = 1e-6\n","        vel_cosangle = torch.sum(velocities_pre * velocities, dim=-1) / ((torch.norm(velocities_pre, dim=-1) + EPS) * (torch.norm(velocities, dim=-1) + EPS))\n","        vel_angle = torch.acos(torch.clamp(vel_cosangle, -1, 1))\n","\n","        batch_size, num_agents, _, _ = coords.shape\n","\n","        valid_agent_mask = self.create_valid_mask2(num_valid, num_agents).type_as(node_features)   # It indicates which agents are valid in the current batch, helping in filtering out invalid data \n","\n","        # Applying DCT transform to coordinates and velocities to transform them into the frequency domain \n","        if self.use_dct:\n","            coords, coords_center, velocities, idct_matrix = self.apply_dct(coords, velocities, valid_agent_mask, num_agents, num_valid, batch_size)\n","\n","        # Creating embedding of node features and velocity angles using learned linear transformations \n","        node_features = self.node_embedding(node_features)\n","        vel_angle_embedding = self.angle_embedding(vel_angle)\n","        # node_features is the feature vector that will be passed to the feature learning layer. It is a combination of the node features and the velocity angle embeddings \n","        node_features = torch.cat([node_features, vel_angle_embedding], dim=-1)\n","\n","        # Normalizing and transforming the coordinates and velocities to account for batch-wise variantions and prepare them for further preprocessing \n","        coords_mean = torch.mean(torch.mean(coords * valid_agent_mask, dim=-2, keepdim=True), dim=-3, keepdim=True) * (num_agents / num_valid[:, None, None, None])\n","        coords = self.coord_transform((coords - coords_mean).transpose(2, 3)).transpose(2, 3) + coords_mean\n","        velocities = self.velocity_transform(velocities.transpose(2, 3)).transpose(2, 3)\n","        coord_velocity_combined = torch.cat([coords, velocities], dim=-2)\n","\n","        valid_mask = self.create_valid_mask(num_valid, num_agents).type_as(node_features)\n","        \n","        # Determines the interaction categories for each edge. If categories are predefined, they are processed accordingly; otherwise, they are computed based on node features and edge attributes.\n","        category = F.one_hot(((edge_attr / 2) + 1).long(), num_classes=self.num_categories) if self.given_category else self.compute_interaction_categories(node_features, coord_velocity_combined, valid_mask)\n","        \n","        # Iteratively applying Feature Learning Layers to update the node features and coordinates based on the interaction categories\n","        category_per_layer = []\n","        for gcl in self.gcls:\n","            node_features, coords, _ = gcl(node_features, coords, velocities, valid_mask, valid_agent_mask, num_valid, category=category)\n","            category_per_layer.append(category)\n","        \n","        # Creating an index for all pairs of nodes to define edges and applies the final graph convolution layer to the node features using these edges \n","        edge_index = torch.combinations(torch.arange(num_agents), r=2).t().to(self.device)\n","        edge_index = torch.cat([edge_index, edge_index.flip(0)], dim=1)\n","        node_features = self.gcl(node_features, edge_index)\n","        \n","        # If recurrent processing is enabled, node features are processed trough an LSTM layer to capture temporal dependencies\n","        if self.use_recurrent:\n","            node_features = node_features.view(batch_size * num_agents, -1, self.hidden_dim)\n","            node_features, _ = self.lstm(node_features)\n","            node_features = node_features.view(batch_size, num_agents, -1, self.hidden_dim)\n","        \n","        # Useing multiple prediction heads to generate outputs. Each head processes the node features to predict the coordinates, and the results are adjusted for mean and combined \n","        all_out = []\n","        for i, (head, head_linear) in enumerate(zip(self.predict_heads, self.predict_heads_linear)):\n","            _, out, _ = head(node_features, coords, velocities, valid_mask, valid_agent_mask, num_valid, category=None)\n","            out_mean = torch.mean(torch.mean(out * valid_agent_mask, dim=-2, keepdim=True), dim=-3, keepdim=True) * (num_agents / num_valid[:, None, None, None])\n","            out = head_linear((out - out_mean).transpose(2, 3)).transpose(2, 3) + out_mean\n","            all_out.append(out[:, :, None, :, :])\n","        \n","        # Concatenating the outputs from all prediction heads and reshapes them to the final output format \n","        coords = torch.cat(all_out, dim=2).view(batch_size, num_agents, 20, self.output_dim, -1)\n","\n","        # If DCT was applied initially, it performs the inverse DCT to transform the coordinates back to the original domain \n","        if self.use_dct:\n","            idct_matrix = idct_matrix[:, :, None, :, :]\n","            coords = torch.matmul(idct_matrix, coords)\n","            coords = coords + coords_center.unsqueeze(2)\n","        \n","        # Returning final predicted coordinates. If validate_reasoning is enabled, it also returns the interaction categories computed during the forward pass \n","        if self.validate_reasoning:\n","            return coords, category_per_layer\n","        else:\n","            return coords, None "]},{"cell_type":"markdown","metadata":{},"source":["### Training "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-19T09:51:10.697547Z","iopub.status.busy":"2024-07-19T09:51:10.697227Z","iopub.status.idle":"2024-07-19T09:51:10.709071Z","shell.execute_reply":"2024-07-19T09:51:10.708195Z","shell.execute_reply.started":"2024-07-19T09:51:10.697522Z"},"trusted":true},"outputs":[],"source":["# Utility Functions\n","def setup_seed(seed):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","def lr_decay(optimizer, lr_now, gamma):\n","    lr_new = lr_now * gamma\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr_new\n","    return lr_new\n","\n","# Save Model Checkpoint\n","def save_checkpoint(epoch, model, optimizer, model_save_dir, subset, best=False):\n","    state = {'epoch': epoch, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n","    file_name = f\"{subset}_ckpt_best.pth.tar\" if best else f\"{subset}_ckpt_{epoch}.pth.tar\"\n","    file_path = os.path.join(model_save_dir, file_name)\n","    torch.save(state, file_path)\n","    \n","# Mask Function for Training\n","def get_valid_mask2(num_valid, agent_num):\n","    batch_size = num_valid.shape[0]\n","    valid_mask = torch.zeros((batch_size, agent_num))\n","    for i in range(batch_size):\n","        valid_mask[i, :num_valid[i]] = 1\n","    return valid_mask.unsqueeze(-1).unsqueeze(-1)\n","\n","# Function to clear cache\n","def clear_cache():\n","    torch.cuda.empty_cache()\n","    \n","def load_and_plot_results(res_path):\n","    # Load the results from file\n","    with open(res_path, 'rb') as f:\n","        data = pickle.load(f)\n","    \n","    train_preds = data['results']['train_preds']\n","    train_gt = data['results']['train_gt']\n","    train_losses = data['results']['train_losses']\n","    val_losses = data['results']['val_losses']\n","\n","    # Plotting losses\n","    plt.figure(figsize=(10, 5))\n","    plt.plot(train_losses, label='Train Loss')\n","    plt.plot(val_losses, label='Validation Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.title('Training and Validation Losses')\n","    plt.show()\n","\n","    return train_preds, train_gt, train_losses, val_losses \n","\n","# Creating animation to show differences between predictions and ground truth labels \n","def create_animation(train_preds, train_gt, train_losses):\n","    fig = plt.figure(figsize=(12, 8))\n","    ax = fig.add_subplot()\n","    plt.axis('off')\n","\n","    def animate(i):\n","        ax.clear()\n","        preds = train_preds[i]\n","        gt = train_gt[i]\n","        x_preds = preds[0, 0, :, 0]   # Adjust indices as per your data shape\n","        y_preds = preds[0, 0, :, 1]\n","        x_gt = gt[0, 0, :, 0]\n","        y_gt = gt[0, 0, :, 1]\n","        ax.scatter(x_preds, y_preds, c='blue', label='Predictions', s=50)\n","        ax.scatter(x_gt, y_gt, c='red', label='Ground Truth', s=50)\n","        plt.title(f'Epoch {i} | Train Loss: {train_losses[i]:.5f}', fontsize=18, pad=20)\n","        plt.legend()\n","        plt.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n","\n","    anim = animation.FuncAnimation(fig, animate, frames=len(train_losses), interval=800, repeat=True)\n","    html = HTML(anim.to_html5_video())\n","    plt.close()\n","    return html"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-19T09:51:10.746286Z","iopub.status.busy":"2024-07-19T09:51:10.746039Z","iopub.status.idle":"2024-07-19T09:51:10.761577Z","shell.execute_reply":"2024-07-19T09:51:10.760743Z","shell.execute_reply.started":"2024-07-19T09:51:10.746265Z"},"trusted":true},"outputs":[],"source":["def train(model, optimizer, epoch, loader, device, backprop=True):\n","    all_predictions = []\n","    all_gt = []\n","    \n","    start_time = time.time()\n","    model.train() if backprop else model.eval()\n","    res = {'epoch': epoch, 'loss': 0, 'counter': 0}\n","    \n","    for batch_idx, data in enumerate(loader):\n","        if data is not None:\n","            pre_data, fut_data, num_valid = data\n","            pre_data, fut_data, num_valid = pre_data.to(device), fut_data.to(device), num_valid.to(device).type(torch.int)\n","            \n","            vel = torch.zeros_like(pre_data).to(device)\n","            vel[:, :, 1:] = pre_data[:, :, 1:] - pre_data[:, :, :-1]\n","            vel[:, :, 0] = vel[:, :, 1]\n","            \n","            batch_size, agent_num, length, _ = pre_data.size()\n","            optimizer.zero_grad()\n","            \n","            nodes = torch.sqrt(torch.sum(vel ** 2, dim=-1)).detach()\n","            loc_pred, category = model(nodes, pre_data.detach(), vel, num_valid, agent_num)\n","            fut_data = fut_data[:, :, None, :, :]\n","            \n","            if args.supervise_all:\n","                mask = get_valid_mask2(num_valid, pre_data.size(1)).to(device)[:, :, None, :, :]\n","                loss = torch.mean(torch.min(torch.mean(torch.norm(mask * (loc_pred - fut_data), dim=-1), dim=3), dim=2)[0])\n","                # if args.supervise_all, the loss is the mean of the minimum norms of the difference between predicted and true locations\n","            else:\n","                loss = torch.mean(torch.min(torch.mean(torch.norm(loc_pred[:, 0:1] - fut_data[:, 0:1], dim=-1), dim=-1), dim=-1)[0])\n","                # if not args.supervise_all, the loss is computed only for the first agent (ego agent)\n","            \n","            if backprop:\n","                loss.backward()\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)  # Clip gradients to avoid explosion\n","                optimizer.step()\n","\n","            res['loss'] += loss.item() * batch_size\n","            res['counter'] += batch_size\n","            \n","            all_predictions.append(loc_pred.detach().cpu())\n","            all_gt.append(fut_data.detach().cpu())\n","    \n","    avg_loss = res['loss'] / res['counter']\n","    epoch_time = time.time() - start_time\n","    \n","    all_predictions = torch.cat(all_predictions, dim=0)\n","    all_gt = torch.cat(all_gt, dim=0)\n","\n","    print(f\"{'==> ' if not backprop else ''}epoch {epoch} avg train loss: {avg_loss:.5f}, time taken: {epoch_time:.2f} seconds\")\n","    return avg_loss, all_predictions, all_gt\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-19T09:51:10.763038Z","iopub.status.busy":"2024-07-19T09:51:10.762712Z","iopub.status.idle":"2024-07-19T09:51:10.776527Z","shell.execute_reply":"2024-07-19T09:51:10.775669Z","shell.execute_reply.started":"2024-07-19T09:51:10.763013Z"},"trusted":true},"outputs":[],"source":["def validate(model, optimizer, epoch, loader, device):\n","    start_time = time.time()\n","    model.eval()\n","    res = {'epoch': epoch, 'loss': 0, 'counter': 0, 'ade': 0}\n","    \n","    with torch.no_grad():\n","        for batch_idx, data in enumerate(loader):\n","            if data is not None:\n","                pre_data, fut_data, num_valid = data\n","                pre_data, fut_data, num_valid = pre_data.to(device), fut_data.to(device), num_valid.to(device).type(torch.int)\n","\n","                vel = torch.zeros_like(pre_data).to(device)\n","                vel[:, :, 1:] = pre_data[:, :, 1:] - pre_data[:, :, :-1]\n","                vel[:, :, 0] = vel[:, :, 1]\n","                \n","                batch_size, agent_num, length, _ = pre_data.size()\n","                optimizer.zero_grad()\n","                \n","                nodes = torch.sqrt(torch.sum(vel ** 2, dim=-1)).detach()\n","                loc_pred, category_list = model(nodes, pre_data.detach(), vel, num_valid, agent_num)\n","\n","\n","                loc_pred = loc_pred.cpu().numpy()\n","                fut_data = fut_data.cpu().numpy()[:, :, None, :, :]\n","                ade = np.mean(np.min(np.mean(np.linalg.norm(loc_pred[:, 0:1] - fut_data[:, 0:1], axis=-1), axis=-1), axis=-1))\n","                # ade measures the average distance between predicted and ground truth locations over all time steps. It is computed as the mean of the minimum error across time steps \n","                fde = np.mean(np.min(np.mean(np.linalg.norm(loc_pred[:, 0:1, :, -1:] - fut_data[:, 0:1, :, -1:], axis=-1), axis=-1), axis=-1))\n","                # fde measures the average distance between predicted and ground truth locations over all time steps. It is computed as the mean of the minimum error across time steps\n","                \n","                res['loss'] += fde*batch_size\n","                res['ade'] += ade*batch_size\n","                res['counter'] += batch_size\n","                \n","    res['ade'] *= args.test_scale\n","    res['loss'] *= args.test_scale\n","    epoch_time = time.time() - start_time\n","    print(f\"==> epoch {epoch} avg val loss: {res['loss'] / res['counter']:.5f} ade: {res['ade'] / res['counter']:.5f}, time taken: {epoch_time:.2f} seconds\")\n","    \n","    return res['loss'] / res['counter'], res['ade'] / res['counter']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-19T09:51:10.778223Z","iopub.status.busy":"2024-07-19T09:51:10.777948Z","iopub.status.idle":"2024-07-19T09:51:10.791162Z","shell.execute_reply":"2024-07-19T09:51:10.790416Z","shell.execute_reply.started":"2024-07-19T09:51:10.778201Z"},"trusted":true},"outputs":[],"source":["def test(model, loader, device):\n","    start_time = time.time()\n","    model.eval()\n","    res = {'loss': 0, 'counter': 0, 'ade': 0}\n","\n","    with torch.no_grad():\n","        for batch_idx, data in enumerate(loader):\n","            if data is not None:\n","                pre_data, fut_data, num_valid = data\n","                pre_data, fut_data, num_valid = pre_data.to(device), fut_data.to(device), num_valid.to(device).type(torch.int)\n","\n","                vel = torch.zeros_like(pre_data).to(device)\n","                vel[:, :, 1:] = pre_data[:, :, 1:] - pre_data[:, :, :-1]\n","                vel[:, :, 0] = vel[:, :, 1]\n","                \n","                batch_size, agent_num, length, _ = pre_data.size()\n","                #optimizer.zero_grad()\n","                \n","                nodes = torch.sqrt(torch.sum(vel ** 2, dim=-1)).detach()\n","                loc_pred, category_list = model(nodes, pre_data.detach(), vel, num_valid, agent_num)\n","                \n","                loc_pred = loc_pred.cpu().numpy()\n","                fut_data = fut_data.cpu().numpy()[:, :, None, :, :]\n","                ade = np.mean(np.min(np.mean(np.linalg.norm(loc_pred[:, 0:1] - fut_data[:, 0:1], axis=-1), axis=-1), axis=-1))\n","                # ade measures the average distance between predicted and ground truth locations over all time steps. It is computed as the mean of the minimum error across time steps\n","                fde = np.mean(np.min(np.mean(np.linalg.norm(loc_pred[:, 0:1, :, -1:] - fut_data[:, 0:1, :, -1:], axis=-1), axis=-1), axis=-1))\n","                # fde measures the average distance between predicted and ground truth locations over all time steps. It is computed as the mean of the minimum error across time steps\n","                \n","                res['loss'] += fde*batch_size\n","                res['ade'] += ade*batch_size\n","                res['counter'] += batch_size\n","                \n","    res['ade'] *= args.test_scale\n","    res['loss'] *= args.test_scale\n","    epoch_time = time.time() - start_time\n","    print(f\"Test avg loss: {res['loss'] / res['counter']:.5f} ade: {res['ade'] / res['counter']:.5f}, time taken: {epoch_time:.2f} seconds\")\n","\n","    \n","    return  res['loss'] / res['counter'], res['ade'] / res['counter']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-19T09:51:10.796536Z","iopub.status.busy":"2024-07-19T09:51:10.796274Z","iopub.status.idle":"2024-07-19T09:51:10.803601Z","shell.execute_reply":"2024-07-19T09:51:10.802750Z","shell.execute_reply.started":"2024-07-19T09:51:10.796514Z"},"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda\" if args.cuda else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-19T09:51:10.814213Z","iopub.status.busy":"2024-07-19T09:51:10.813908Z","iopub.status.idle":"2024-07-19T09:51:10.831185Z","shell.execute_reply":"2024-07-19T09:51:10.830281Z","shell.execute_reply.started":"2024-07-19T09:51:10.814173Z"},"trusted":true},"outputs":[],"source":["# Final results function\n","def final_results():\n","    # Seed setup\n","    if args.seed >= 0:\n","        seed = args.seed\n","        setup_seed(seed)\n","    else:\n","        seed = random.randint(0, 1000)\n","        setup_seed(seed)\n","    print('The seed is:', seed)\n","\n","    # Model setup\n","    model = EquiPredict(\n","        node_features=args.past_length, \n","        edge_features=2, \n","        hidden_dim=args.nf, \n","        input_dim=args.past_length, \n","        hidden_channel_dim=args.channels, \n","        output_dim=args.future_length, \n","        device=device, \n","        act_fn=nn.SiLU(), \n","        layers=args.n_layers, \n","        coords_weight=1.0, \n","        use_recurrent=False, \n","        normalize_diff=False, \n","        use_tanh=args.tanh\n","    )\n","\n","    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n","\n","    results = {'epochs': [], 'train_losses': [], 'val_losses': [], 'train_preds': [], 'train_gt': []}\n","    best_val_loss = 1e8\n","    best_val_ade = 1e8\n","    best_epoch = 0\n","    lr_now = args.lr\n","\n","    for epoch in range(0, args.epochs):\n","        # Apply learning rate decay if specified\n","        if args.apply_decay:\n","            if epoch % args.epoch_decay == 0 and epoch > 0:\n","                lr_now = lr_decay(optimizer, lr_now, args.lr_gamma)\n","        \n","        # Train the model\n","        train_loss, train_preds, train_gt = train(model, optimizer, epoch, loader_train, device)\n","        results['train_losses'].append(train_loss)\n","        results['train_preds'].append(train_preds)\n","        results['train_gt'].append(train_gt)\n","        print(f'Epoch {epoch}: Train Loss: {train_loss:.5f}')\n","        \n","        # Evaluate on validation set\n","        val_loss, val_ade = validate(model, optimizer, epoch, loader_val, device)\n","        results['val_losses'].append(val_loss)\n","        print(f'Epoch {epoch}: Validation Loss: {val_loss:.5f}, Validation ADE: {val_ade:.5f}')\n","        \n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_val_ade = val_ade\n","            best_epoch = epoch\n","            state = {'epoch': epoch,\n","                     'state_dict': model.state_dict(),\n","                     'optimizer': optimizer.state_dict()}\n","            file_path = os.path.join(args.model_save_dir, f'{args.subset}_ckpt_best.pth')\n","            torch.save(state, file_path)\n","\n","        clear_cache()\n","\n","        # Save intermediate results to reduce memory usage\n","        if epoch % 5 == 0:  # Adjust frequency as needed\n","            results_path = os.path.join('/kaggle/working/saved_models', f'training_results_epoch_{epoch}.pkl')\n","            with open(results_path, 'wb') as f:\n","                pickle.dump({'results': results}, f)\n","            clear_cache()\n","\n","    # Save the final model at the end of the training and validation phases \n","    state = {'epoch': epoch, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n","    file_path = os.path.join(args.model_save_dir, f'{args.subset}_final.pth')\n","    torch.save(state, file_path)\n","    \n","    # Load the trained model\n","    model_trained_path = file_path \n","    print('Loading model from:', model_trained_path)\n","    model_ckpt = torch.load(model_trained_path)\n","    model.load_state_dict(model_ckpt['state_dict'], strict=False)\n","    test_loss, ade = test(model, loader_test, device)\n","    print('ADE final:', ade, 'FDE final:', test_loss)\n","    \n","    # Save the final results dictionary\n","    results_path = os.path.join('/kaggle/working/saved_models', 'training_results.pkl')\n","    with open(results_path, 'wb') as f:\n","        pickle.dump({'results': results}, f)\n","\n","    clear_cache()\n","    return results"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-19T09:51:10.832740Z","iopub.status.busy":"2024-07-19T09:51:10.832341Z","iopub.status.idle":"2024-07-19T10:01:22.996973Z","shell.execute_reply":"2024-07-19T10:01:22.996031Z","shell.execute_reply.started":"2024-07-19T09:51:10.832708Z"},"trusted":true},"outputs":[],"source":["results = final_results()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-20T04:52:50.198641Z","iopub.status.busy":"2024-07-20T04:52:50.197679Z","iopub.status.idle":"2024-07-20T04:52:50.202899Z","shell.execute_reply":"2024-07-20T04:52:50.201978Z","shell.execute_reply.started":"2024-07-20T04:52:50.198606Z"},"trusted":true},"outputs":[],"source":["# Save the results dictionary and other outputs\n","res_path = os.path.join('/kaggle/working/saved_models', f'training_results_outside.pkl')\n","with open(res_path, 'wb') as f:\n","        pickle.dump({\n","            'results': results\n","        }, f)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_preds, train_gt, train_losses, val_losses = load_and_plot_results(res_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-19T10:12:20.107935Z","iopub.status.busy":"2024-07-19T10:12:20.107109Z","iopub.status.idle":"2024-07-19T10:12:20.862658Z","shell.execute_reply":"2024-07-19T10:12:20.861723Z","shell.execute_reply.started":"2024-07-19T10:12:20.107900Z"},"trusted":true},"outputs":[],"source":["def create_animation(train_preds, train_gt, train_losses):\n","    fig = plt.figure(figsize=(12, 8))\n","    ax = fig.add_subplot()\n","    plt.axis('off')\n","\n","    def animate(i):\n","        ax.clear()\n","        preds = train_preds[i]\n","        gt = train_gt[i]\n","        x_preds = preds[0, 0, :, 0]   # Adjust indices as per your data shape\n","        y_preds = preds[0, 0, :, 1]\n","        x_gt = gt[0, 0, :, 0]\n","        y_gt = gt[0, 0, :, 1]\n","        ax.scatter(x_preds, y_preds, c='blue', label='Predictions', s=50)\n","        ax.scatter(x_gt, y_gt, c='red', label='Ground Truth', s=50)\n","        plt.title(f'Epoch {i} | Train Loss: {train_losses[i]:.5f}', fontsize=18, pad=20)\n","        plt.legend()\n","        plt.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n","\n","    anim = animation.FuncAnimation(fig, animate, frames=len(train_losses), interval=800, repeat=True)\n","    html = HTML(anim.to_html5_video())\n","    plt.close()\n","    return html"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["animation_html = create_animation(train_preds, train_gt, train_losses)\n","display(animation_html)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5327773,"sourceId":8851097,"sourceType":"datasetVersion"},{"datasetId":5413905,"sourceId":8988995,"sourceType":"datasetVersion"}],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
